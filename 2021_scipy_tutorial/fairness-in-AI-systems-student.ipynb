{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nokks/Fairness/blob/main/2021_scipy_tutorial/fairness-in-AI-systems-student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDxtnoIr7mIN"
      },
      "source": [
        "# SciPy 2021 Tutorial <br>_Fairness in AI systems: <br>From social context to practice using Fairlearn_\n",
        "\n",
        "---\n",
        "\n",
        "_SciPy 2021 Tutorial: Fairness in AI systems: From social context to practice using Fairlearn by Manojit Nandi, Miroslav Dudík, Triveni Gandhi, Lisa Ibañez, Adrin Jalali, Michael Madaio, Hanna Wallach, Hilde Weerts is licensed under\n",
        "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sch9KDWg7SL8"
      },
      "source": [
        "Fairness in AI systems is an interdisciplinary field of research and practice that aims to understand and address some of the negative impacts of AI systems on society. In this tutorial, we will walk through the process of assessing and mitigating fairness-related harms in the context of the U.S. health care system. This tutorial will consist of a mix of instructional content and hands-on demonstrations using Jupyter notebooks. Participants will use the Fairlearn library to assess ML models for performance disparities across different racial groups and mitigate those disparities using a variety of algorithmic techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fboVvqvVvpKz"
      },
      "source": [
        "# **Prepare environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0p461hmgrmz"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5Nsb2Rgux7"
      },
      "source": [
        "Note that the runtime environment needs to be restarted after installing `model-card-toolkit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_uVNMHUdb2w",
        "outputId": "addadc2e-d41d-478d-80ee-1d8c06843600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairlearn==0.7.0\n",
            "  Downloading fairlearn-0.7.0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from fairlearn==0.7.0) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from fairlearn==0.7.0) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from fairlearn==0.7.0) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from fairlearn==0.7.0) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.1->fairlearn==0.7.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.1->fairlearn==0.7.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.1->fairlearn==0.7.0) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->fairlearn==0.7.0) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->fairlearn==0.7.0) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.1->fairlearn==0.7.0) (1.16.0)\n",
            "Installing collected packages: fairlearn\n",
            "Successfully installed fairlearn-0.7.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Installing collected packages: seaborn\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.1\n",
            "    Uninstalling seaborn-0.13.1:\n",
            "      Successfully uninstalled seaborn-0.13.1\n",
            "Successfully installed seaborn-0.13.2\n",
            "Collecting model-card-toolkit\n",
            "  Downloading model_card_toolkit-2.0.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py<1.1,>=0.9 (from model-card-toolkit)\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2<3.2,>=3.1 in /usr/local/lib/python3.10/dist-packages (from model-card-toolkit) (3.1.3)\n",
            "Requirement already satisfied: matplotlib<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from model-card-toolkit) (3.7.1)\n",
            "Collecting jsonschema<4,>=3.2.0 (from model-card-toolkit)\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-data-validation<2.0.0,>=1.5.0 (from model-card-toolkit)\n",
            "  Downloading tensorflow_data_validation-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.42.0,>=0.36.0 (from model-card-toolkit)\n",
            "  Downloading tensorflow_model_analysis-0.41.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata<2.0.0,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from model-card-toolkit) (1.14.0)\n",
            "Collecting ml-metadata<2.0.0,>=1.5.0 (from model-card-toolkit)\n",
            "  Downloading ml_metadata-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from absl-py<1.1,>=0.9->model-card-toolkit) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=3.1->model-card-toolkit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4,>=3.2.0->model-card-toolkit) (23.2.0)\n",
            "Collecting pyrsistent>=0.14.0 (from jsonschema<4,>=3.2.0->model-card-toolkit)\n",
            "  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from jsonschema<4,>=3.2.0->model-card-toolkit) (67.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (2.8.2)\n",
            "Collecting attrs>=17.4.0 (from jsonschema<4,>=3.2.0->model-card-toolkit)\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.8.6 in /usr/local/lib/python3.10/dist-packages (from ml-metadata<2.0.0,>=1.5.0->model-card-toolkit) (1.62.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.10/dist-packages (from ml-metadata<2.0.0,>=1.5.0->model-card-toolkit) (3.20.3)\n",
            "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow-data-validation<2.0.0,>=1.5.0->model-card-toolkit)\n",
            "  Downloading apache_beam-2.55.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<2.0.0,>=1.5.0->model-card-toolkit) (1.4.0)\n",
            "Collecting pandas<2,>=1.0 (from tensorflow-data-validation<2.0.0,>=1.5.0->model-card-toolkit)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow<11,>=10 (from tensorflow-data-validation<2.0.0,>=1.5.0->model-card-toolkit)\n",
            "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyfarmhash<0.4,>=0.2.2 (from tensorflow-data-validation<2.0.0,>=1.5.0->model-card-toolkit)\n",
            "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow<3,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<2.0.0,>=1.5.0->model-card-toolkit) (2.15.0)\n",
            "Collecting tfx-bsl<1.15,>=1.14.0 (from tensorflow-data-validation<2.0.0,>=1.5.0->model-card-toolkit)\n",
            "  Downloading tfx_bsl-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata<2.0.0,>=1.5.0->model-card-toolkit) (1.63.0)\n",
            "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.42.0,>=0.36.0->model-card-toolkit) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.42.0,>=0.36.0->model-card-toolkit) (7.7.1)\n",
            "INFO: pip is looking at multiple versions of tensorflow-model-analysis to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-model-analysis<0.42.0,>=0.36.0 (from model-card-toolkit)\n",
            "  Downloading tensorflow_model_analysis-0.41.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_model_analysis-0.40.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_model_analysis-0.39.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_model_analysis-0.38.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_model_analysis-0.37.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_model_analysis-0.36.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py<1.1,>=0.9 (from model-card-toolkit)\n",
            "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib<4,>=3.2.0 (from model-card-toolkit)\n",
            "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow-model-analysis to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing<3.1,>=2.3.1 (from matplotlib<4,>=3.2.0->model-card-toolkit)\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib<4,>=3.2.0 (from model-card-toolkit)\n",
            "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading matplotlib-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools-scm>=4 (from matplotlib<4,>=3.2.0->model-card-toolkit)\n",
            "  Downloading setuptools_scm-8.0.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib<4,>=3.2.0 (from model-card-toolkit)\n",
            "  Downloading matplotlib-3.4.3.tar.gz (37.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.4.2.tar.gz (37.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.4.1.tar.gz (37.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.4.0.tar.gz (37.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.1/37.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.3.4.tar.gz (37.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.3.3.tar.gz (37.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.3.2.tar.gz (37.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.2.0->model-card-toolkit) (2024.2.2)\n",
            "  Downloading matplotlib-3.3.1.tar.gz (38.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.3.0.tar.gz (38.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.2.2.tar.gz (40.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.2.1.tar.gz (40.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading matplotlib-3.2.0.tar.gz (40.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting model-card-toolkit\n",
            "  Downloading model_card_toolkit-1.3.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of model-card-toolkit to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading model_card_toolkit-1.3.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2<3,>=2.10 (from model-card-toolkit)\n",
            "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting model-card-toolkit\n",
            "  Downloading model_card_toolkit-1.3.0-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading model_card_toolkit-1.2.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py<0.11,>=0.9 (from model-card-toolkit)\n",
            "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting model-card-toolkit\n",
            "  Downloading model_card_toolkit-1.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading model_card_toolkit-1.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading model_card_toolkit-0.1.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading model_card_toolkit-0.1.2-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version<3,>=2.8.0 (from model-card-toolkit)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "INFO: pip is looking at multiple versions of model-card-toolkit to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting model-card-toolkit\n",
            "  Downloading model_card_toolkit-0.1.1-py3-none-any.whl (38 kB)\n",
            "Collecting absl-py<0.9,>=0.7 (from model-card-toolkit)\n",
            "  Downloading absl-py-0.8.1.tar.gz (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade fairlearn==0.7.0\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade seaborn\n",
        "!pip install model-card-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVtpwFGJLcgU"
      },
      "source": [
        "## Import and set up packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cEwLsWyTLgJn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.float_format\", \"{:.3f}\".format)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYgrI99Nxrai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wBOSsyc48MK0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U setuptools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NVBt07IxXK6",
        "outputId": "fda7ef55-5aba-435b-b3b0-43f0105e1071"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD6Im8Aqyjgu",
        "outputId": "9c8eab87-573b-4a09-d76b-fe177388efd5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VhbFzU6GLgW0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import Bunch\n",
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    RocCurveDisplay)\n",
        "from sklearn import set_config\n",
        "\n",
        "set_config(display=\"diagram\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GcilMnWhLgaT"
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import (\n",
        "    MetricFrame,\n",
        "    true_positive_rate,\n",
        "    false_positive_rate,\n",
        "    false_negative_rate,\n",
        "    selection_rate,\n",
        "    count,\n",
        "    false_negative_rate_difference\n",
        ")\n",
        "\n",
        "from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer\n",
        "from fairlearn.postprocessing._interpolated_thresholder import InterpolatedThresholder\n",
        "from fairlearn.postprocessing._threshold_operation import ThresholdOperation\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, TruePositiveRateParity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oMbCFBSZr2A4"
      },
      "outputs": [],
      "source": [
        "# Model Card Toolkit works in Google Colab, but it does not work on all local environments\n",
        "# that we tested. If the import fails, define a dummy function in place of the function\n",
        "# for saving figures into images in a model card..\n",
        "\n",
        "try:\n",
        "    from model_card_toolkit import ModelCardToolkit\n",
        "    from model_card_toolkit.utils.graphics import figure_to_base64str\n",
        "    model_card_imported = True\n",
        "except Exception:\n",
        "    model_card_imported = False\n",
        "    def figure_to_base64str(*args):\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S4ANMj1M8k0h"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "from datetime import date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OoPTetsDv-v"
      },
      "source": [
        "# **Overview of fairness in AI systems**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt9n43o6DPbg"
      },
      "source": [
        "Please refer to the slides here: https://raw.githubusercontent.com/fairlearn/talks/main/2021_scipy_tutorial/overview.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gZTFumIxCfK"
      },
      "source": [
        "# **Introduction of Fairlearn and other tutorial resources**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KhsuCB-imFk"
      },
      "source": [
        "This tutorial builds on the following open source projects:\n",
        "\n",
        "* **machine learning and data processing**: _scikit-learn_, _pandas_, _numpy_\n",
        "* **plotting**: _seaborn_, _matplotlib_\n",
        "* **AI fairness**: _Fairlearn_, _Model Card Toolkit_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORfTAukTuEPN"
      },
      "source": [
        "### [Fairlearn](https://fairlearn.org)\n",
        "\n",
        "Fairlearn is an open-source, community-driven project to help data scientists improve fairness of AI systems. It includes:\n",
        "\n",
        "* A Python library for fairness assessment and improvement (fairness metrics, mitigation algorithms, plotting, etc.)\n",
        "\n",
        "* Educational resources covering organizational and technical processes for unfairness mitigation (user guide, case studies, Jupyter notebooks, etc.)\n",
        "\n",
        "The project was started in 2018 at Microsoft Research. In 2021 it adopted neutral governance structure and since then it is completely community-driven."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE3bFABst9ze"
      },
      "source": [
        "### [Model Card Toolkit](https://github.com/tensorflow/model-card-toolkit)\n",
        "\n",
        "The Model Card Toolkit (MCT) streamlines and automates generation of _model cards_, machine learning documents that provide context and transparency into a model's development and performance. It was released by Google in 2020."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j1vtg6TD7Fi"
      },
      "source": [
        "# **Introduction to the health care scenario**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUkG_zdEylGU"
      },
      "source": [
        "Our scenario builds on previous research that highlighted racial disparities in how health care resources are allocated in the U.S. ([Obermeyer et al., 2019](https://science.sciencemag.org/content/366/6464/447.full)).\n",
        "Motivated by that work, in this tutorial we consider an automated system for recommending patients for _high-risk care management_ programs, which are described by Obermeyer et al. 2019 as follows:\n",
        "\n",
        "> These programs seek to improve the care of patients with complex health needs by providing additional resources, including greater attention from trained providers, to help ensure that care is well coordinated. Most health systems use these programs as the cornerstone of population health management efforts, and they are widely considered effective at improving outcomes and satisfaction while reducing costs. [...] Because the programs are themselves expensive—with costs going toward teams of dedicated nurses, extra primary care appointment slots, and other scarce resources—**health systems rely extensively on algorithms to identify patients who will benefit the most.**\n",
        "\n",
        "**Convenience restriction**\n",
        "\n",
        "* In practice, the modeling of health needs would use large data sets covering a wide range of diagnoses. In this tutorial, we will work with a [publicly available clinical dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008) that focuses on _diabetic patients only_ ([Strack et al., 2014](https://www.hindawi.com/journals/bmri/2014/781670/))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-j4KN95wLLS"
      },
      "source": [
        "## Dataset and task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOwrRsB7wEeM"
      },
      "source": [
        "We will be working with a clincial dataset of hospital re-admissions over a ten-year period (1998-2008) for diabetic patients across 130 different hospitals in the US. Each record represents the hospital admission records for a patient diagnosed with diabetes whose stay lasted one to fourteen days.\n",
        "\n",
        "The features describing each encounter include demographics, diagnoses, diabetic medications, number of visits in the year preceding the encounter, and payer information, as well as whether the patient was readmitted after release, and whether the readmission occurred within 30 days of the release.\n",
        "\n",
        "We would like to develop a classification model, which decides whether the patients should be suggested to their primary care physicians for an enrollment into the high-risk care management program. The positive prediction will mean recommendation into the care program.\n",
        "\n",
        "**Decision point: Task definition**\n",
        "\n",
        "* A hospital **readmission within 30 days** can be viewed as a proxy that the patients needed more assistance at the release time, so it will be the label we wish to predict.\n",
        "\n",
        "* Because of the class imbalance, we will be measuring our performance via **balanced accuracy**. Another key performance consideration is how many patients are recommended for care, metric we refer to as **selection rate**.\n",
        "\n",
        "Ideally, health care professionals would be involved in both designing and using the model, including formalizing the task definition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BE26iXWwUqr"
      },
      "source": [
        "## Fairness considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZUcQVZYyRvz"
      },
      "source": [
        "* _Which groups are most likely to be disproportionately negatively affected?_ Previous work suggests that groups with different race and ethnicity can be differently affected.\n",
        "\n",
        "* _What are the harms?_ The key harms here are allocation harms. In particular, false negatives, i.e., don't recommend somebody who will be readmitted.\n",
        "\n",
        "* _How should we measure those harms?_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kD6G-yF1Tcf"
      },
      "source": [
        "In the remainder of the tutorial we will:\n",
        "* First examine the dataset and our choice of label with an eye towards a variety of fairness issues.\n",
        "* Then train a logistic regression model and assess its performance as well as fairness.\n",
        "* Finally, look at two unfairness mitigation strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkTmOAh8Bp1D"
      },
      "source": [
        "<a name=\"discussion-fairness-issues\"></a>\n",
        "## Discussion: Fairness-related harms\n",
        "\n",
        "* How can we determine which type of harm is relevant in a particular scenario?\n",
        "* What are ways to find out which (groups of) individuals are most likely to be disproportionately negatively affected?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7tpRumX_4Nh"
      },
      "source": [
        "# Task definition and dataset characteristics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iJGhfCgPiy-"
      },
      "source": [
        "Two critical decisions when desiging an AI system are\n",
        "1. how we define the machine learning task\n",
        "2. what dataset we use to train our models\n",
        "\n",
        "These choices are often intertwined, because the dataset is often a convenience dataset, based on availability, which leads to a specific choice of label and performance metric (that's also the case in our scenario).\n",
        "\n",
        "In this part of the tutorial, we first load the dataset, and then we examine it for a variety of fairness issues:\n",
        "1. sample sizes of different demographic groups, and in particular different racial groups\n",
        "2. appropriateness of our choice of label (readmission within 30 days)\n",
        "3. representativeness/informativeness of different features for different groups\n",
        "\n",
        "Besides dataset characteristics, one additional aspect of dataset fairness is whether the data was collected in a manner that respects the autonomy of individuals in the dataset.\n",
        "\n",
        "The dataset characteristics can be systematically documented through the **datasheets** practice. We will touch on this later on. By documenting our understanding of the dataset, we communicate any concerns we have about the data and highlight downstream issues that may arise during the model training, evaluation and deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-ABnntZT8Fn"
      },
      "source": [
        "## Load the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvyHqcLIT8Fo"
      },
      "source": [
        "We next load the dataset and review the meaning of its columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkfwniFQT8Fp"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/fairlearn/talks/main/2021_scipy_tutorial/data/diabetic_preprocessed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIFN96kiT8Fq"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXQNItgRT8Fu"
      },
      "source": [
        "The columns contain mostly boolean and categorical data (including age and various test results), with just the following exceptions: `time_in_hospital`, `num_lab_procedures`, `num_procedures`, `num_medications`, `number_diagnoses`.\n",
        "\n",
        "\n",
        "|features| description|\n",
        "|---|---|\n",
        "| race, gender, age | demographic features |\n",
        "| medicare, medicaid | insurance information |\n",
        "| admission_source_id | emergency, referral, or other |\n",
        "| had_emergency, had_inpatient_days,<br>had_outpatient_days | hospital visits in prior year |\n",
        "| medical_specialty | admitting physician's specialty |\n",
        "| time_in_hospital, num_lab_procedures,<br>num_procedures, num_medications,<br>primary_diagnosis, number_diagnoses,<br>max_glu_serum, A1Cresult, insulin<br>change, diabetesMed | description of the hospital visit<br> |\n",
        "| discharge_disposition_id | discharched to home or not |\n",
        "| readmitted, readmit_binary,<br>readmit_30_days | readmission information |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPtaDcfHT8Fv"
      },
      "outputs": [],
      "source": [
        "# Show the values of all binary and categorical features\n",
        "categorical_values = {}\n",
        "for col in df:\n",
        "  if col not in {'time_in_hospital', 'num_lab_procedures',\n",
        "                 'num_procedures', 'num_medications', 'number_diagnoses'}:\n",
        "    categorical_values[col] = pd.Series(df[col].value_counts().index.values)\n",
        "categorical_values_df = pd.DataFrame(categorical_values).fillna('')\n",
        "categorical_values_df.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8loEiuFSWb8A"
      },
      "source": [
        "We mark all categorical features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4y1FRMdWduE"
      },
      "outputs": [],
      "source": [
        "categorical_features = [\n",
        "    \"race\",\n",
        "    \"gender\",\n",
        "    \"age\",\n",
        "    \"discharge_disposition_id\",\n",
        "    \"admission_source_id\",\n",
        "    \"medical_specialty\",\n",
        "    \"primary_diagnosis\",\n",
        "    \"max_glu_serum\",\n",
        "    \"A1Cresult\",\n",
        "    \"insulin\",\n",
        "    \"change\",\n",
        "    \"diabetesMed\",\n",
        "    \"readmitted\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyo3GQ4RWduG"
      },
      "outputs": [],
      "source": [
        "for col_name in categorical_features:\n",
        "  df[col_name] = df[col_name].astype(\"category\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LTax67Em4q8"
      },
      "source": [
        "## Group sample sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft28kXKHm4q9"
      },
      "source": [
        "From the perspective of fairness assessment, a key data characteristic is the sample size of groups with respect to which we conduct fairness assessment.\n",
        "\n",
        "Small sample sizes have two implications:\n",
        "\n",
        "* **assessment**: the impacts of the AI system on smaller groups are harder to assess, because due to fewer data points we have a much larger uncertainty (error bars) in our estimates\n",
        "\n",
        "* **model training**: fewer training data points mean that our model fails to appropriately capture any data patterns specific to smaller groups, which means that its predictive performance on these groups could be worse\n",
        "\n",
        "Let's examine the sample sizes of the groups according to `race`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEGJjCrOm4q9"
      },
      "outputs": [],
      "source": [
        "df[\"race\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HznSDLWEm4q-"
      },
      "outputs": [],
      "source": [
        "df[\"race\"].value_counts().plot(kind='bar', rot=45);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkI6KA3rm4q-"
      },
      "source": [
        "Normalized as frequencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE2UoDU3m4q-"
      },
      "outputs": [],
      "source": [
        "df[\"race\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEUrg6J3m4q_"
      },
      "source": [
        "In our dataset, our patients are predominantly *Caucasian* (75%). The next largest racial group is *AfricanAmerican*, making up 19% of the patients. The remaining race categories (including *Unknown*) compose only 6% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIdDhgtAm4rA"
      },
      "source": [
        "We also examine the dataset composition by `gender`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h26CH50m4rA"
      },
      "outputs": [],
      "source": [
        "df[\"gender\"].value_counts() # counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTbI5v48m4rA"
      },
      "outputs": [],
      "source": [
        "df[\"gender\"].value_counts(normalize=True) # frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3xBANdpm4rA"
      },
      "source": [
        "Gender is in our case effectively binary (and we have no further information how it was operationalized), with both *Female* represented at 54% and *Male* represented at 46%. There are only 3 samples annotated as *Unknown/Invalid*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyLBXpKWm4rA"
      },
      "source": [
        "### Decision point: How do we address smaller group sizes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT-BPQ2Gm4rB"
      },
      "source": [
        "When the data set lacks coverage of certain groups, it means that we will not be able to reliably assess any fairness-related issues. There are three interventions (which could be carried out in a combination):\n",
        "\n",
        "* **collect more data**: collect more data for groups with fewer samples\n",
        "* **buckets**: merge some of the groups\n",
        "* **drop small groups**\n",
        "\n",
        "The choice of strategy depends on our existing understanding of which groups are at the greatest risk of a harm. In particular, pooling the groups with widely different risks could mask the extent of harms. We generally caution against dropping small groups as this leads to the representational harm of erasure.\n",
        "\n",
        "If any groups are merged or dropped, these decisions should be annotated / explained (in the datasheet, which we discuss below).\n",
        "\n",
        "In our case, we will:\n",
        "\n",
        "* merge the three smallest race groups *Asian*, *Hispanic*, *Other* (similar to [Strack et al., 2014](https://www.hindawi.com/journals/bmri/2014/781670/)), but also retain the original groups for auxiliary assessments\n",
        "\n",
        "* drop the gender group *Unknown/Invalid*, because the sample size is so small that no meaningful fairness assessment is possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBN_fIhpm4rB"
      },
      "outputs": [],
      "source": [
        "# drop gender group Unknown/Invalid\n",
        "df = df.query(\"gender != 'Unknown/Invalid'\")\n",
        "\n",
        "# retain the original race as race_all, and merge Asian+Hispanic+Other\n",
        "df[\"race_all\"] = df[\"race\"]\n",
        "df[\"race\"] = df[\"race\"].replace({\"Asian\": \"Other\", \"Hispanic\": \"Other\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Sb8ISAnRQF"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns1Tr9wLm4rB"
      },
      "source": [
        "Please examine the distribution of the `age` feature in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDiLp-cDoWGv"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOn4j1o8m4rB"
      },
      "outputs": [],
      "source": [
        "df[\"age\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns7RxP5um4rB"
      },
      "outputs": [],
      "source": [
        "df[\"age\"].value_counts().plot(kind='bar', rot=0);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAXa7MoQnjq4"
      },
      "source": [
        "As we might expect, most patients admitted into the hospital in our data set belong to the *Over 60 years* category. Although we will not be assessing for age-based fairness-related harms in this tutorial, we will want to document the age imbalance in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK0LbF_sylTU"
      },
      "source": [
        "## Examining the choice of label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtgLr5Cs_YhF"
      },
      "source": [
        "Next we dive into the question of whether our choice of label (readmission within 30 days) aligns with our goal (identify patients that would benefit from the care management program).\n",
        "\n",
        "A framework particularly suited for this analysis is called _measurement modeling_ (see, e.g., [Jacobs and Wallach, 2021](https://arxiv.org/abs/1912.05511)). The goal of measurement modeling is to describe the relationship between what we care about and what we can measure. The thing that we care about is referred to as the _construct_ and what we can observe is referred to as the _measurement_. In our case:\n",
        "* **construct** = greatest benefit from the care management program\n",
        "* **measurement** = readmission within 30 days (in the absence of such program)\n",
        "\n",
        "In our case, the **measurement** coincides with the **classification label**.\n",
        "\n",
        "The act of _operationalizing_ the construct via a specific measurement corresponds to making certain assumptions. In our case, we are making the following assumption: **the greatest benefit from the care management program would go to patients that are** (in the absence of such a program) **most likely to be readmitted into the hospital within 30 days.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA19isovvCew"
      },
      "source": [
        "### How can we check whether our assumptions apply?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY95_kOFO6Wb"
      },
      "source": [
        "In the terminology of measurement modeling, how do we establish _construct validity_? Following, [Jacobs and Wallach, 2021](https://arxiv.org/abs/1912.05511),\n",
        "\n",
        "> Establishing construct\n",
        "validity means demonstrating, in a variety of ways, that the measurements obtained from measurement model are both meaningful\n",
        "and useful:\n",
        "> * Does the operationalization capture all relevant aspects\n",
        "of the construct purported to be measured?\n",
        "> * Do the measurements\n",
        "look plausible?\n",
        "> * Do they correlate with other measurements of the\n",
        "same construct? Or do they vary in ways that suggest that the\n",
        "operationalization may be inadvertently capturing aspects of other\n",
        "constructs?\n",
        "> * Are the measurements predictive of measurements of\n",
        "any relevant observable properties (and other unobservable theoretical constructs) thought to be related to the construct, but not incorporated into the operationalization?\n",
        "\n",
        "We focus on one aspect of construct validity, called _predictive validity_, which refers to the extent\n",
        "to which the measurements obtained from a measurement model\n",
        "are predictive of measurements of any relevant observable properties\n",
        "related to the construct purported to be measured, but not incorporated into the operationalization.\n",
        "\n",
        "The predictions do not need to be chronological, meaning that we do not necessarily need to be predicting future from the past. Also, the predictions do not need to be causal (going from causes to effects). We just need to ensure that the predicted property is not part of the measurement whose validity we're checking.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IBUG3Sa96LX"
      },
      "source": [
        "### Predictive validity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVrLpuwy98uG"
      },
      "source": [
        "We would like to show that our measurement `readmit_30_days` is correlated with patient characteristics that are related to our construct \"benefiting from care management\". One such characteristic is the general patient health, where we expect that patients that are less healthy are more likely to benefit from care management.\n",
        "\n",
        "While our data does not contain full health records that would enable us to holistically measure general patient health, the data does contain two relevant features: `had_emergency` and `had_inpatient_days`, which indicate whether the patient spent any days in the emergency room or in the hospital (but non-emergency) in the preceding year.\n",
        "\n",
        "To establish predictive validity, we would like to show that our measurement `readmit_30_days` is predictive of these two observable characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQWxJEN-M6VD"
      },
      "source": [
        "First, let's check the rate at which the patients with different `readmit_30_days` labels were readmitted in the previous year:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t11OgTgZfV8"
      },
      "outputs": [],
      "source": [
        "sns.pointplot(y=\"had_emergency\", x=\"readmit_30_days\",\n",
        "              data=df, ci=95, join=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptl-tHkDf_GJ"
      },
      "source": [
        "The plot shows that indeed patients with `readmit_30_days=0` have a lower rate of emergency visits in the prior year, whereas patients with `readmit_30_days=1` have a larger rate. (The vertical lines indicate 95% confidence intervals obtained via boostrapping.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07GU8IGIY9KC"
      },
      "source": [
        "We see a similar pattern when `readmit_30_days` is used to predict the rate of (non-emergency) hospital visits in the previous year:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuPPVpXrO5uE"
      },
      "outputs": [],
      "source": [
        "sns.pointplot(y=\"had_inpatient_days\", x=\"readmit_30_days\",\n",
        "              data=df, join=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN8NU8QkRMqM"
      },
      "source": [
        "Now let's take a look whether the predictiveness is similar across different race groups. First, let's check how well `readmit_30_days` predicts `had_emergency`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgLhUlZzOvHC"
      },
      "outputs": [],
      "source": [
        "# Visualize predictiveness using a categorical pointplot\n",
        "sns.catplot(y=\"had_emergency\", x=\"readmit_30_days\", hue=\"race\", data=df,\n",
        "            kind=\"point\", ci=95, dodge=True, join=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ7zH3Wn416g"
      },
      "source": [
        "The patients in the group *Unknown* have a substantially lower rate of emergency visits in the prior year, regardless of whether they are readmitted in 30 days. The readmission is still positively correlated with `had_emergency`, but note the large error bars (due to small sample sizes).\n",
        "\n",
        "We also see that the group with feature value *AfricanAmerican* has a higher rate of emergency visits compared with other groups. However, generally the groups *Caucasian*, *AfricanAmerican* and *Other* follow similar dependence patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G13abbdS7oM-"
      },
      "source": [
        "We see a similar pattern when `readmit_30_days` is used to predict the rate of (non-emergency) hospital visits in the previous year:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx52yWYNQuSH"
      },
      "outputs": [],
      "source": [
        "sns.catplot(y=\"had_inpatient_days\", x=\"readmit_30_days\", hue=\"race\", data=df,\n",
        "            kind=\"point\", ci=95, dodge=True, join=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrCelj5_hR6a"
      },
      "source": [
        "Again, for *Unknown* the rate of (non-emergency) hospital visits in the previous year is lower than for other groups.In all groups there is a strong positive correlation between `readmit_30_days` and `had_inpatient_days`.\n",
        "\n",
        "In all cases, we see that readmission in 30 days is predictive of our two measurements of general patient health.\n",
        "\n",
        "The analysis is also surfacing the fact that patients with the value of race *Unknown* have fewer hospital visits in the preceding year (both emergency and non-emergency) than other groups. In practice, this would be a good reason to reach out to health professionals to investigate this patient cohort, to make sure that we understand why there is the systematic difference.\n",
        "\n",
        "Note that we have only investigated _predictive validity_, but there are other important aspects of construct validity which we may want to establish (see [Jacobs and Wallach, 2021](https://arxiv.org/abs/1912.05511))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_4CZt-UBaOQ"
      },
      "source": [
        "<a name=\"exercise-predictive-validity\"></a>\n",
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW9ktRlqBhZA"
      },
      "source": [
        "Check the predictive validity with respect to `gender` and `age`. Do you see any differences? Can you form a hypothesis why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLom8GFbUfqR"
      },
      "outputs": [],
      "source": [
        "# Check for predictive validity by gender\n",
        "sns.catplot(y=\"had_inpatient_days\",x=\"readmit_30_days\",hue=_____, data=df,\n",
        "            kind=\"point\", ci=95, dodge=True, join=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PdFAu3lT-pD"
      },
      "outputs": [],
      "source": [
        "# Check for predictive validity by age\n",
        "sns.catplot(y=\"had_inpatient_days\", x=\"readmit_30_days\", hue=_____, data=df,\n",
        "            kind=\"point\", ci=95, dodge=True, join=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i9KdmTWKUJZ"
      },
      "source": [
        "## Label imbalance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViGqA5VTGrEo"
      },
      "source": [
        "Now that we have established the validity of our label, we will check frequency of its values in our data. The frequency of different labels is an important descriptive characteristic in classification settings for several reasons:\n",
        "\n",
        "* some classification algorithms and performance measures might not work well with data sets with extreme class imbalance\n",
        "* in binary classification settings, our ability to evaluate error is often driven by the size of the smaller of the two classes (again, the smaller the sample the larger the uncertainty in estimates)\n",
        "* label imbalance may exacerbate the problems due to smaller group sizes in fairness assessment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cos3--59EiZt"
      },
      "source": [
        "Let's check how many samples in our data are labeled as positive and how many as negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR5ULLYGE4UK"
      },
      "outputs": [],
      "source": [
        "df[\"readmit_30_days\"].value_counts() # counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UWBbg4Cz90t"
      },
      "outputs": [],
      "source": [
        "df[\"readmit_30_days\"].value_counts(normalize=True) # frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-_K_KHXz9MR"
      },
      "source": [
        "As we can see, the target label is heavily skewed towards the patients not being readmitted within 30 days. In our dataset, only 11% of patients were readmitted within 30 days.\n",
        "\n",
        "Since there are fewer positive examples, we expect that we will have a much larger uncertainty (error bars) in our estimates of *false negative rates* (FNR), compared with *false positive rates* (FPR). This means that there will be larger differences between training FNR and test FNR, even if there is no overfitting, simply because of the smaller sample sizes.\n",
        "\n",
        "Our target metric is *balanced error rate*, which is the average of FPR and FNR. The value of this metric is robust to different frequencies of positives and negatives. However, since half of the metric is contributed by FNR, we expect the uncertainty in balanced error values to behave similarly to the uncertainty of FNR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkopoyE3GQ8g"
      },
      "source": [
        "Now, let's examine how much the label frequencies vary within each group defined by `race`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcH3Mxwmm4rC"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x=\"readmit_30_days\", y=\"race\", data=df, ci=95);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox06aTMmm4rB"
      },
      "source": [
        "We see the rate of *30-day readmission* is similar for the *AfricanAmerican* and *Caucasian* groups, but appears smaller for *Other* and smallest for *Unknown* (this is consistent with an overall lower rate of hospital visits in the prior year). The smaller sample size of the *Other* and *Unknown* groups mean that there is more uncertainty around the estimate for these two groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AA5uoqAKUSx"
      },
      "source": [
        "## Proxies for sensitive features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PSup7dMJjJg"
      },
      "source": [
        "We next investigate which of the features are highly predictive of the sensitive feature *race*; such features are called *proxies*.\n",
        "\n",
        "While in this tutorial we examine fairness issues through the **impact** of the machine-learning model on different populations, there are other concepts of fairness that seek to analyze how the **model might be using information** contained in the sensitive features, and which of the information uses are justified (often using causal reasoning). More pragmatically, certain uses of sensitive features (or proxies of it) might be illegal in some contexts.\n",
        "\n",
        "Another reason to understand the proxies is because they might explain why we see differences in impact on different groups even when our model does not have access to the sensitive features directly.\n",
        "\n",
        "In this section we briefly examine the identification of such proxies (but we don't go into legal or causality considerations).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PHULa3eQEcn"
      },
      "source": [
        "In the United States, *Medicare* and *Medicaid* are joint federal and state programs to help qualified individuals pay for healthcare expenses. *Medicare* is available to people over the age of 65 and younger individuals with severe illnesses. *Medicaid* is available to all individuals under the age of 65 whose adjusted gross income falls below the Federal Poverty Line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNYx0OaVElUC"
      },
      "source": [
        "First, let's explore the relationship between patients who paid with *Medicaid* and our demographic features. Because *Medicaid* is available to low-income individuals, and race is correlated with socioeconomic status in the United States, we expect there to be a relationship between `race` and paying with *Medicaid*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR3eNMeY0HTi"
      },
      "outputs": [],
      "source": [
        "sns.pointplot(y=\"medicaid\", x=\"race\", data=df, join=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTcAD0oxbarW"
      },
      "source": [
        "From our analysis, we see that paying with *Medicaid* does appear to have some relationship with the patient's race. *Caucasian* patients are the least likely to pay with *Medicaid* compared with other groups. If paying with *Medicaid* is a proxy for socioeconomic status, then the patterns we find align with our understanding of wealth and race in the United States."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BLSciTKaBLd"
      },
      "source": [
        "## Additional validity checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PzKOA59ezFs"
      },
      "source": [
        "Similarly as we used predictive validity to check that our label aligns with the construct of \"likely to benefit from the care management program\", we can use predictive validity to verify that our various features are coherent with each other.\n",
        "\n",
        "For example, based on the eligibility criteria for *Medicaid* vs *Medicare*, we expect `medicaid` to be negatively correlated with age and `medicare` to be positively correlated with age:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "869rzUjSUe3C"
      },
      "outputs": [],
      "source": [
        "sns.pointplot(y=\"medicaid\", x=\"age\", data=df, ci=95, join=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnI8lFHFZyBL"
      },
      "outputs": [],
      "source": [
        "sns.pointplot(y=\"medicare\", x=\"age\", data=df, ci=95, join=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBgmuoz8aXw4"
      },
      "source": [
        "As we see, that's indeed the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdrS4By1dVk-"
      },
      "source": [
        "<a name=\"exercise-dataset\"></a>\n",
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-sOl0rqblsb"
      },
      "source": [
        "Now, let's explore the relationship between paying with `medicare` and other demographic features. In the below sections, feel free to perform any analysis you would like to better understand the relationship between `medicare` and `race` and `gender` in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp02oUbWZx54"
      },
      "outputs": [],
      "source": [
        "sns.pointplot(y=\"medicare\", x=____, data=df, ci=95, join=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONJcedCWULXN"
      },
      "outputs": [],
      "source": [
        "sns.pointplot(y=\"medicare\", x=____, data=df, ci=95, join=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXtY7_xdULLe"
      },
      "outputs": [],
      "source": [
        "sns.pointplot(y=\"medicare\", x=____, data=df, ci=95, join=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVSerRqDG3we"
      },
      "source": [
        "<a name=\"datasheets\"></a>\n",
        "## Datasheets for datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACLMWSAwGaLc"
      },
      "source": [
        "The _datasheets_ practice was proposed by [Gebru et al. (2018)](https://arxiv.org/abs/1803.09010). A datasheet of a given dataset documents the motivation behind the dataset creation, the dataset composition, collection process, recommended uses and many other characteristics. In the words of Gebru et al., the goal is to\n",
        "> facilitate better communication between dataset creators\n",
        "> and dataset consumers, and encourage the machine learning\n",
        "> community to prioritize transparency and accountability.\n",
        "\n",
        "In this section, we show how to fill in some of the sections of the datasheet for the dataset that we are using. The information is obtained directly from [Strack et al. (2014)](https://www.hindawi.com/journals/bmri/2014/781670/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcc4cUMhZnCb"
      },
      "source": [
        "### Example sections of a datasheet [OPTIONAL SECTION]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TIPJIhX1IKJ"
      },
      "source": [
        "**For what purpose was the dataset created?** *Was there a specific task in mind? Was there a specific gap that needed to be filled?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBNKbKJQJbmf"
      },
      "source": [
        "In the words of the dataset authors:\n",
        "> [...] the management of hyperglycemia in the hospitalized patient has a significant bearing on outcome, in terms of both morbidity and mortality. This recognition has led to the development of formalized protocols in the intensive care unit (ICU) setting [...] However, the same cannot be said for most non-ICU inpatient admissions. [...] there are few national assessments of diabetes care in the hospitalized patient which could serve as a baseline for change [in the non-ICU protocols]. The present analysis of a large clinical database was undertaken to examine historical patterns of diabetes care in patients with diabetes admitted to a US hospital and to inform future directions which might lead to improvements in patient safety."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA-XRu0o1ErL"
      },
      "source": [
        "**Who created the dataset (e.g., which team) and on behalf of which entity?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQSHxl26LQkt"
      },
      "source": [
        "The dataset was created by [Strack et al. (2014)](https://www.hindawi.com/journals/bmri/2014/781670/): a team of researchers from a variety of disciplines, ranging from computer science to public health, from three institutions (Virginia Commonwealth University, University of Cordoba, and Polish Academy of Sciences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qx32mSJG3zP"
      },
      "source": [
        "#### **Composition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RS2V8001F3E"
      },
      "source": [
        "**What do the instances that comprise the dataset represent?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPy_TXp_1Gub"
      },
      "source": [
        "Each instance in this dataset represents a hospital admission for diabetic patient (diabetes was entered as a possible diagnosis for the patient) whose hospital stay lasted between one to fourteen days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOb0FPeOJqxm"
      },
      "source": [
        "**Is any information missing from individual instances?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vlZWeQjJq8w"
      },
      "source": [
        "The features `Payer Code` and `Medical Specialty` have 40,255 and 49,947 missing values, respectively. For `Payer Code`, these missing values are reflected in the category *Unknown*. For `Medical Specialty`, these missing values are reflecting in the category *Missing*.\n",
        "\n",
        "For our demographic features, we are missing the `Gender` information for three patients in the dataset. These three records were dropped from our final dataset. Regarding `Race`, the 2,271 missing values were recoded into the `Unknown` race category.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh0lLV6mJrSp"
      },
      "source": [
        "**Does the dataset identify any subpopulations (e.g., by age, gender)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-ZnQfibJrcQ"
      },
      "source": [
        "Patients are identified by gender, age group, and race.\n",
        "\n",
        "For gender, patients are identified as Male, Female, or Unknown. There were only three instances where the patient gender is *Unknown*, so these records were removed from our dataset.\n",
        "\n",
        "Gender | Count| Percentage\n",
        "------ | ------|----------\n",
        "Male      | 47055     | 46.2%\n",
        "Female     |  54708     | 53.7%  \n",
        "\n",
        "\n",
        "\n",
        "For age group, patients are binned into three age buckets: *30 years or younger*, *30-60 years*, *Older than 60 years*.\n",
        "\n",
        "Age Group |Count| Percentage\n",
        "------ | ------|----------\n",
        "30 years or younger      | 2509     | 2.4%\n",
        "30-60 years       | 30716   | 30.2%\n",
        "Older than 60 years      |   68538    | 67.4%  \n",
        "\n",
        "\n",
        "For race, patients are identified as *AfricanAmerican*, *Caucasian*, and *Other*. For individuals whose race information was not collected during hospital admission, their race is listed as *Unknown*.\n",
        "\n",
        "Race | Count| Percentage\n",
        "------ | ------|----------\n",
        "Caucasian      | 76099     | 74.8%\n",
        "AfricanAmerican     |  19210     | 18.9%  \n",
        "Other        |     4183         |  4.1%\n",
        "Unknown        |    2271          | 2.2%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzXw0egqG4J4"
      },
      "source": [
        "#### **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGfxGcI21Fyj"
      },
      "source": [
        "**Was any preprocessing/cleaning/labeling of the data done?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jO4Pf911GrL"
      },
      "source": [
        "For the `race` feature, the categories of *Asian* and *Hispanic* and *Other* were merged into the *Other* category. The `age` feature was bucketed into 30-year intervals (*30 years and below*, *30 to 60 years*, and *Over 60 years*). The `discharge_disposition_id` was binarized into a boolean outcome on whether an patient was discharged to home.\n",
        "\n",
        "The full preprocessing code is provided in the file `preprocess.py` of the tutorial [GitHub repository](https://github.com/fairlearn/talks/blob/main/2021_scipy_tutorial/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8b5nXfPIA7a"
      },
      "source": [
        "#### **Uses**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YH8evvN1HX2"
      },
      "source": [
        "**Has the dataset been used for any tasks already?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8RW1LKW1Hbg"
      },
      "source": [
        "This dataset has been used by [Strack et al. (2014)](https://www.hindawi.com/journals/bmri/2014/781670/) to model the relationship between patient readmission and HbA1c measurement during admission, based on primary medical diagnosis.\n",
        "\n",
        "The dataset is publicly available through the UCI Machine Learning Repository and, as of May 2021, has received over 350,000 views."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB7zfhA1UKiW"
      },
      "source": [
        "# Training the initial model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f7jZOzGX24Z"
      },
      "source": [
        "We next train a classification model to predict our target variable (readmission within 30 days) while optimizing balanced accuracy.\n",
        "\n",
        "What kind of model should we train? Deep neural nets? Random forests? Logistic regression?\n",
        "\n",
        "There are a variety of considerations. We highlight two:\n",
        "\n",
        "* **Interpretability.** Interpretability is tightly linked with questions of fairness. There are several reasons why it is important to have interpretable models that are open to the stakeholder scrutiny:\n",
        "  * It allows discovery of fairness issues that were not discovered by the data science team.\n",
        "  * It provides a path toward recourse for those that are affected by the model.\n",
        "  * It allows for a *face validity* check, a \"sniff test\", by experts to verify that the model \"makes sense\" (at the face value). While this step is subjective, it is really important when the model is applied to different populations than those on which the assessment was conducted.\n",
        "\n",
        "* **Model expressiveness.** How well can the model separate positive examples from negative examples? How well can it do so given the available dataset size? Can it do so across all groups or does it need to trade off performance on one group against performance on another group?\n",
        "\n",
        "Some additional considerations are training time (this impacts the ability to iterate), familiarity (this impacts the ability to fine tune and debug), and carbon footprint (this impacts the Earth climate both directly and indirectly by normalizing unnecessarily heavy workloads)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hbsqXap9Mzp"
      },
      "source": [
        "### Decision point: Model type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iftAwdfoVDM0"
      },
      "source": [
        "We will use a logistic regression model. Our reasoning:\n",
        "\n",
        "* **Interpretability**. Logistic models over a small number of variables (as used here) are highly interpretable in the sense that stakeholders can simulate them easily.\n",
        "\n",
        "* **Model expressiveness**. Logistic regression predictions are described by a linear weighting of the feature values. The concern might be that this is too simple. The previous work by [Strack et al. (2014)](https://www.hindawi.com/journals/bmri/2014/781670/), which also used a logistic model to predict readmission rates concluded that a slightly more expressive model might be useful (their analysis uncovered 8 pairwise interactions that were significant, see their Table 5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52wWLOFoXkho"
      },
      "source": [
        "## Prepare training and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCwtDyx8yqSG"
      },
      "source": [
        "As we mentioned in the task definition, our target variable is **readmission within 30 days**, and our sensitive feature for the purposes of fairness assessment is **race**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpiSRj2JyqSH"
      },
      "outputs": [],
      "source": [
        "target_variable = \"readmit_30_days\"\n",
        "demographic = [\"race\", \"gender\"]\n",
        "sensitive = [\"race\"]\n",
        "# If multiple sensitive features are chosen, the rest of the script considers intersectional groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaNqDyqvi1QE"
      },
      "outputs": [],
      "source": [
        "Y, A = df.loc[:, target_variable], df.loc[:, sensitive]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niu49A9YXQmf"
      },
      "source": [
        "We next drop the features that we don't want to use in our model and expand the categorical features into 0/1 indicators (\"dummies\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXyRuCsri1cY"
      },
      "outputs": [],
      "source": [
        "X = pd.get_dummies(df.drop(columns=[\n",
        "    \"race\",\n",
        "    \"race_all\",\n",
        "    \"discharge_disposition_id\",\n",
        "    \"readmitted\",\n",
        "    \"readmit_binary\",\n",
        "    \"readmit_30_days\"\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAA0sIhQUWFa"
      },
      "outputs": [],
      "source": [
        "X.head() # sanity check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATzi8cKCD7V3"
      },
      "source": [
        "We split our data into a training and test portion. The test portion will be used to evaluate our performance metric (i.e., balanced accuracy), but also for fairness assessment. The split is half/half for training and test to ensure that we have sufficient sample sizes for fairness assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wpnURazmJ4-"
      },
      "outputs": [],
      "source": [
        "random_seed = 445\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgl_b-CUl7TW"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test, A_train, A_test, df_train, df_test = train_test_split(\n",
        "    X,\n",
        "    Y,\n",
        "    A,\n",
        "    df,\n",
        "    test_size=0.50,\n",
        "    stratify=Y,\n",
        "    random_state=random_seed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSMbXR9iVqr8"
      },
      "source": [
        "Our performance metric is **balanced accuracy**, so for the purposes of training (but not evaluation!) we will resample the data set, so that it has the same number of positive and negative examples. This means that we can use estimators that optimize standard accuracy (although some estimators allow the use us importance weights).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPNQpb2ZN1ku"
      },
      "source": [
        "Because we are downsampling the number of negative examples, we create a training dataset with a significantly lower number of data points. For more complex machine learning models, this lower number of training data points may affect the model's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1aVgzyFNa4B"
      },
      "outputs": [],
      "source": [
        "def resample_dataset(X_train, Y_train, A_train):\n",
        "\n",
        "  negative_ids = Y_train[Y_train == 0].index\n",
        "  positive_ids = Y_train[Y_train == 1].index\n",
        "  balanced_ids = positive_ids.union(np.random.choice(a=negative_ids, size=len(positive_ids)))\n",
        "\n",
        "  X_train = X_train.loc[balanced_ids, :]\n",
        "  Y_train = Y_train.loc[balanced_ids]\n",
        "  A_train = A_train.loc[balanced_ids, :]\n",
        "  return X_train, Y_train, A_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ogw-r3DQsds"
      },
      "outputs": [],
      "source": [
        "X_train_bal, Y_train_bal, A_train_bal = resample_dataset(X_train, Y_train, A_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRddJS7XXv5n"
      },
      "source": [
        "## Save descriptive statistics of training and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ-T4lGxX0IQ"
      },
      "source": [
        "We next evaluate and save descriptive statistics of the training dataset. These will be provided as part of _model cards_ to document our training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3GhcUCm2LjD"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=\"race\", data=A_train_bal)\n",
        "plt.title(\"Sensitive Attributes for Training Dataset\")\n",
        "sensitive_train = figure_to_base64str(plt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIp3j8fD2LjE"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=Y_train_bal)\n",
        "plt.title(\"Target Label Histogram for Training Dataset\")\n",
        "outcome_train = figure_to_base64str(plt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czIGZYhk2LjF"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=\"race\", data=A_test)\n",
        "plt.title(\"Sensitive Attributes for Testing Dataset\")\n",
        "sensitive_test = figure_to_base64str(plt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjhW0t9-2LjF"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=Y_test)\n",
        "plt.title(\"Target Label Histogram for Test Dataset\")\n",
        "outcome_test = figure_to_base64str(plt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V523GQbYobT"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7jwN2cVbO0g"
      },
      "source": [
        "We train a logistic regression model and save its predictions on test data for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6nKDzt164vw"
      },
      "outputs": [],
      "source": [
        "unmitigated_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessing\", StandardScaler()),\n",
        "    (\"logistic_regression\", LogisticRegression(max_iter=1000))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld9clGbHl7tv"
      },
      "outputs": [],
      "source": [
        "unmitigated_pipeline.fit(X_train_bal, Y_train_bal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok-eREU0xbAD"
      },
      "outputs": [],
      "source": [
        "Y_pred_proba = unmitigated_pipeline.predict_proba(X_test)[:,1]\n",
        "Y_pred = unmitigated_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkA0K8KV0HeD"
      },
      "source": [
        "Check model performance on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz7QJOLx0RVH"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve of probabilistic predictions\n",
        "RocCurveDisplay(unmitigated_pipeline, X_test, Y_test);  # RocCurveDisplay replaced plot_roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxYppCAy1owq"
      },
      "outputs": [],
      "source": [
        "# Show balanced accuracy rate of the 0/1 predictions\n",
        "balanced_accuracy_score(Y_test, Y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmWrDs5N2HVD"
      },
      "source": [
        "As we see, the performance of the model is well above the performance of a coin flip (whose performance would be 0.5 in both cases), albeit it is quite far from a perfect classifier (whose performance would be 1.0 in both cases).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmhwS1Z9VnK9"
      },
      "source": [
        "## Inspect the coefficients of trained model\n",
        "\n",
        "We check the coefficients of the fitted model to make sure that they \"makes sense\". While subjective, this step is important and helps catch mistakes and might point out to some fairness issues. However, we will systematically assess the fairness of the model in the next section.\n",
        "\n",
        "*Note that coefficients are also a proxy for \"feature importance\", but this interpretation can be misleading when features are highly correlated.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owzkar8R9Cyy"
      },
      "outputs": [],
      "source": [
        "coef_series = pd.Series(data=unmitigated_pipeline.named_steps[\"logistic_regression\"].coef_[0], index=X.columns)\n",
        "coef_series.sort_values().plot.barh(figsize=(4, 12), legend=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX8CrWjhD7MB"
      },
      "source": [
        "# **Fairness assessment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CS9-jaxtxh2"
      },
      "source": [
        "## Measuring fairness-related harms\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8TMm9w8duVY"
      },
      "source": [
        "The goal of fairness assessment is to answer the question: *Which groups of people may be disproportionately negatively impacted by an AI system and in what ways?*\n",
        "\n",
        "The steps of the assesment are as follows:\n",
        "1. Identify harms\n",
        "2. Identify the groups that might be harmed\n",
        "3. Quantify harms\n",
        "4. Compare quantified harms across the groups\n",
        "\n",
        "We next examine these four steps in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6hMFmzmPbL6"
      },
      "source": [
        "### 1. Identify harms\n",
        "\n",
        "For example, in a system for screening job applications, qualified candidates that are automatically rejected experience an allocation harm. In a speech-to-text transcription system, high error rates constitute harm in the quality of service.\n",
        "\n",
        "**In the health care scenario**, the patients that would benefit from a care management program, but are not recommended for it experience an allocation harm. In the context of the classification scenario these are **FALSE NEGATIVES**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqqUk1mjPnpM"
      },
      "source": [
        "### 2. Identify the groups that might be harmed\n",
        "\n",
        "In most applications, we consider demographic groups including historically marginalized groups (e.g., based on gender, race, ethnicity). We should also consider groups that are relevant to a particular application. For example, for speech-to-text transcription, groups based on the regional dialect or being a native or a non-native speaker.\n",
        "\n",
        "It is also important to consider group intersections, for example, in addition to considering groups according to gender and groups according to race, it is also important to consider their intersections (e.g., Black women, Latinx nonbinary people, etc.).\n",
        "\n",
        "**In the health care scenario**, based on the previous work, we focus on groups defined by **RACE**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmvSqI3dPrVk"
      },
      "source": [
        "### 3. Quantify harms\n",
        "\n",
        "Define metrics that quantify harms or benefits:\n",
        "\n",
        "* In job screening scenario, we need to quantify the number of candidates that are classified as \"negative\" (not recommended for the job), but whose true label is \"positive\" (they are qualified). One possible metric is the **false negative rate**: fraction of qualified candidates that are screened out.\n",
        "\n",
        "* In speech-to-text scenario, the harm could be measured by **word error rate**, number of mistakes in a transcript divided by the overall number of words.\n",
        "\n",
        "* **In the health care scenario**, we could consider two metrics for quantifying harms / benefits:\n",
        "  * **false negative rate**: fraction of patients that are readmitted within 30 days, but that are not recommended for the care management program; this quantifies harm\n",
        "  * **selection rate**: overall fraction of patients that are recommended for the care management program (regardless of whether they are readmittted with 30 days or no); this quantifies benefit; here the assumption is that all patients benefit similarly from the extra care.\n",
        "\n",
        "There are several reasons for including selection rate in addition to false negative rate. We would like to monitor how the benefits are allocated, focusing on groups that might be disadvantaged. Another reason is to get extra robustness in our assessement, because our measure (i.e., readmission within 30 days) is only an imperfect measure of our construct (who is most likely to benefit from the care management program). The auxiliary metrics, like selection rate, may alert us to large disparities in how the benefit is allocated, and allow us to catch issues that we might have missed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpJXt6miPvRX"
      },
      "source": [
        "### 4. Compare quantified harms across the groups\n",
        "\n",
        "The workhorse of fairness assessment are _disaggregated metrics_, which are **metrics evaluated on slices of data**. For example, to measure harms due to errors, we would begin by evaluating the errors on each slice of the data that corresponds to a group we identified in Step 2.\n",
        "If some of the groups are seeing much larger errors than other groups, we would flag this as a fairness harm.\n",
        "\n",
        "To summarize the disparities in errors (or other metrics), we may want to report quantities such as the **difference** or **ratio** of the metric values between the best and the worst slice. In settings where the goal is to guarantee certain minimum quality of service (such as speech recognition), it is also meaningful to report the **worst performance** across all considered groups.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Is_zdXvnW0s"
      },
      "source": [
        "For example, when comparing false negative rate across groups defined by race, we may summarize our findings with a table like the following:\n",
        "\n",
        "| | false negative rate<br>(FNR) |\n",
        "|---|---|\n",
        "| AfricanAmerican | 0.43 |\n",
        "| Caucasian | 0.44 |\n",
        "| Other | 0.52 |\n",
        "| Unknown | 0.67 |\n",
        "| | |\n",
        "|_largest difference_| 0.24 &nbsp;&nbsp;(best is 0.0)|\n",
        "|_smallest ratio_| 0.64 &nbsp;&nbsp;(best is 1.0)|\n",
        "|_maximum_<br>_(worst-case) FNR_|0.67|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CjHlopBDgSG"
      },
      "source": [
        "## Fairness assessment with `MetricFrame`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epJO2baHV2Dy"
      },
      "source": [
        "Fairlearn provides the data structure called `MetricFrame` to enable evaluation of disaggregated metrics. We will show how to use a `MetricFrame` object to assess the trained `LogisticRegression` classifier for potential fairness-related harms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iiAYRvoduPh"
      },
      "outputs": [],
      "source": [
        "# In its simplest form MetricFrame takes four arguments:\n",
        "#    metric_function with signature metric_function(y_true, y_pred)\n",
        "#    y_true: array of labels\n",
        "#    y_pred: array of predictions\n",
        "#    sensitive_features: array of sensitive feature values\n",
        "\n",
        "mf1 = MetricFrame(metrics=false_negative_rate,\n",
        "                  y_true=Y_test,\n",
        "                  y_pred=Y_pred,\n",
        "                  sensitive_features=df_test['race'])\n",
        "\n",
        "# The disaggregated metrics are stored in a pandas Series mf1.by_group:\n",
        "\n",
        "mf1.by_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tl1Qxt1fT2v"
      },
      "outputs": [],
      "source": [
        "# The largest difference, smallest ratio and worst-case performance are accessed as\n",
        "#   mf1.difference(), mf1.ratio(), mf1.group_max()\n",
        "\n",
        "print(f\"difference: {mf1.difference():.3}\\n\"\n",
        "      f\"ratio: {mf1.ratio():.3}\\n\"\n",
        "      f\"max across groups: {mf1.group_max():.3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2zmBHo5gk-F"
      },
      "outputs": [],
      "source": [
        "# You can also evaluate multiple metrics by providing a dictionary\n",
        "\n",
        "metrics_dict = {\n",
        "    \"selection_rate\": selection_rate,\n",
        "    \"false_negative_rate\": false_negative_rate,\n",
        "    \"balanced_accuracy\": balanced_accuracy_score,\n",
        "}\n",
        "\n",
        "metricframe_unmitigated = MetricFrame(metrics=metrics_dict,\n",
        "                  y_true=Y_test,\n",
        "                  y_pred=Y_pred,\n",
        "                  sensitive_features=df_test['race'])\n",
        "\n",
        "# The disaggregated metrics are then stored in a pandas DataFrame:\n",
        "\n",
        "metricframe_unmitigated.by_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc29jRJrhlSC"
      },
      "outputs": [],
      "source": [
        "# The largest difference, smallest ratio, and the maximum and minimum values\n",
        "# across the groups are then all pandas Series, for example:\n",
        "\n",
        "metricframe_unmitigated.difference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVbjFa4Aig9Y"
      },
      "outputs": [],
      "source": [
        "# You'll probably want to view them transposed:\n",
        "\n",
        "pd.DataFrame({'difference': metricframe_unmitigated.difference(),\n",
        "              'ratio': metricframe_unmitigated.ratio(),\n",
        "              'group_min': metricframe_unmitigated.group_min(),\n",
        "              'group_max': metricframe_unmitigated.group_max()}).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvjRBIcjjSkl"
      },
      "outputs": [],
      "source": [
        "# You can also easily plot all of the metrics using DataFrame plotting capabilities\n",
        "\n",
        "metricframe_unmitigated.by_group.plot.bar(subplots=True, layout= [1,3], figsize=(12, 4),\n",
        "                      legend=False, rot=-45, position=1.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5C3SITjuPUm"
      },
      "source": [
        "According to the above bar chart, it seems that the group *Unknown* is selected for the care management program less often than other groups as reflected by the selection rate. Also this group experiences the largest false negative rate, so a larger fraction of group members that are likely to benefit from the care management program are not selected. Finally, the balanced accuracy on this group is also the lowest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Qs68rv2_Vg"
      },
      "source": [
        "We observe disparity, even though we did not include race in our model. There's a variety of reasons why such disparities may occur. It could be due to representational issues (i.e., not enough instances per group), or because the feature distribution itself differs across groups (i.e., different relationship between features and target variable, obvious example would be people with darker skin in facial recognition systems, but can be much more subtle). Real-world applications often exhibit both kinds of issues at the same time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_1Rm8PbPmmk"
      },
      "source": [
        "<a name=\"train-other-models\"></a>\n",
        "## Exercise: Train other fairness-unaware models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeQF5qT6Qs-C"
      },
      "source": [
        "In this section, you'll be training your own fairness-unaware model and evaluate the model using the `MetricFrame` for fairness-related harms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsHy-Os0oVQU"
      },
      "source": [
        "We encourage you to explore the model's performance across different sensitive features (such as `age` or `gender`) as well as different model performance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61GU_zrFSC-6"
      },
      "source": [
        "1.) First, let's train our machine learning model. We'll create a `HistGradientBoostingClassifier` and fit it to the balanced training data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSIDkuV4_Rou"
      },
      "outputs": [],
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "# Create your model here\n",
        "clf = HistGradientBoostingClassifier()\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(__________, ________)\n",
        "exercise_pred = clf.predict(______)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnnles-p6OXr"
      },
      "source": [
        "2.) Next, let's evaluate the fairness of the model using the `MetricFrame`. In the below cells, create a `MetricFrame` that looks at the following metrics:\n",
        "\n",
        "\n",
        "*   _Count_: The number of data points belonging to each sensitive feature category.\n",
        "*   _False Positive Rate_: $\\dfrac{FN}{FN+TP}$\n",
        "*  _Recall Score_: $\\dfrac{TP}{TP+FN}$\n",
        "\n",
        "As an extra challenge, you can use the prediction probabilities to compute the _ROC AUC Score_ for each sensitive group pair.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcf-x1oA_jP5"
      },
      "outputs": [],
      "source": [
        "# Define exercise fairness metrics of interest here\n",
        "exercise_metrics = {\n",
        "    \"count\": count,\n",
        "    \"false_positive_rate\": _______,\n",
        "    \"recall_score\": _______\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bll-8GAWJF6p"
      },
      "source": [
        "Now, let's create our `MetricFrame` using the metrics listed above with the sensitive groups of `race` and `gender`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAjzjCqh_fNx"
      },
      "outputs": [],
      "source": [
        "metricframe_exercise = MetricFrame(\n",
        "    metrics=__________,\n",
        "    y_true=Y_test,\n",
        "    y_pred=__________,\n",
        "    sensitive_features=_____\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeghVCbLZOf5"
      },
      "source": [
        "3.) Finally, play around with the plotting capabilities of the `MetricFrame` in the below section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd4D17ME_hB2"
      },
      "outputs": [],
      "source": [
        "metricframe_exercise._______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xaLx6Br_hyc"
      },
      "outputs": [],
      "source": [
        "# Plot some of the performance disparities here\n",
        "metricframe_exercise.by_group.____.bar(subplots=_____, layout=[1,3], figsize=(12, 4),\n",
        "                                       legend=False, rot=-45, position=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me1ocEi2kEgw"
      },
      "source": [
        "The charts above are based on test data, so without any uncertainty quantification (such as error bars or confidence intervals), we cannot reliably compare these data statistics. Next optional section shows how to augment MetricFrame with the report of error bars.\n",
        "\n",
        "## Adding error bars [OPTIONAL SECTION]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l8YJ8qQdehm"
      },
      "source": [
        "In this section, we define new custom metrics that quantify errors in our estimates of selection rate, false negative rate and balanced accuracy, and then review our metrics again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiP-uXr_FLtz"
      },
      "outputs": [],
      "source": [
        "# All of our error bar calculations are based on normal approximation to\n",
        "# the binomial variables.\n",
        "\n",
        "def error_bar_normal(n_successes, n_trials, z=1.96):\n",
        "  \"\"\"\n",
        "  Computes the error bars for the parameter p of a binomial variable\n",
        "  using normal approximation. The default value z corresponds to the 95%\n",
        "  confidence interval.\n",
        "  \"\"\"\n",
        "  point_est = n_successes / n_trials\n",
        "  error_bar = z*np.sqrt(point_est*(1-point_est))/np.sqrt(n_trials)\n",
        "  return error_bar\n",
        "\n",
        "def fpr_error(Y_true, Y_pred):\n",
        "  \"\"\"\n",
        "  Compute the 95%-error bar for the false positive rate\n",
        "  \"\"\"\n",
        "  tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred).ravel()\n",
        "  return error_bar_normal(fp, tn+fp)\n",
        "\n",
        "def fnr_error(Y_true, Y_pred):\n",
        "  \"\"\"\n",
        "  Compute the 95%-error bar for the false negative rate\n",
        "  \"\"\"\n",
        "  tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred).ravel()\n",
        "  return error_bar_normal(fn, fn+tp)\n",
        "\n",
        "def selection_rate_error(Y_true, Y_pred):\n",
        "  \"\"\"\n",
        "  Compute the 95%-error bar for the selection rate\n",
        "  \"\"\"\n",
        "  tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred).ravel()\n",
        "  return error_bar_normal(tp+fp, tn+fp+fn+tp)\n",
        "\n",
        "def balanced_accuracy_error(Y_true, Y_pred):\n",
        "  \"\"\"\n",
        "  Compute the 95%-error bar for the balanced accuracy\n",
        "  \"\"\"\n",
        "  fnr_err, fpr_err = fnr_error(Y_true, Y_pred), fpr_error(Y_true, Y_pred)\n",
        "  return np.sqrt(fnr_err**2 + fpr_err**2)/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHaXfBYWp6ob"
      },
      "source": [
        "We next create a metric frame that includes the sample sizes and error bar sizes in addition to the metrics that we have used previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlEq6ogfyHb6"
      },
      "outputs": [],
      "source": [
        "metrics_with_err_bars = {\n",
        "    \"count\": count,\n",
        "    \"selection_rate\": selection_rate,\n",
        "    \"selection_err_bar\": selection_rate_error,\n",
        "    \"false_negative_rate\": false_negative_rate,\n",
        "    \"fnr_err_bar\": fnr_error,\n",
        "    \"balanced_accuracy\": balanced_accuracy_score,\n",
        "    \"bal_acc_err_bar\": balanced_accuracy_error\n",
        "}\n",
        "\n",
        "# sometimes we will only want to display metrics without error bars\n",
        "metrics_to_display = [\n",
        "    \"count\",\n",
        "    \"selection_rate\",\n",
        "    \"false_negative_rate\",\n",
        "    \"balanced_accuracy\"\n",
        "]\n",
        "\n",
        "# sometimes we will only want to show the difference values of the metrics other than count\n",
        "differences_to_display = [\n",
        "    \"selection_rate\",\n",
        "    \"false_negative_rate\",\n",
        "    \"balanced_accuracy\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55GOqFTYxbCi"
      },
      "outputs": [],
      "source": [
        "metricframe_unmitigated_w_err = MetricFrame(\n",
        "    metrics=metrics_with_err_bars,\n",
        "    y_true=Y_test,\n",
        "    y_pred=Y_pred,\n",
        "    sensitive_features=A_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlZKPNkHxbFb"
      },
      "outputs": [],
      "source": [
        "unmitigated_groups = metricframe_unmitigated_w_err.by_group\n",
        "unmitigated_groups # show both the metrics as well as the error bars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh5F-A5C5rSV"
      },
      "source": [
        "We see that for smaller sample sizes we have larger error bars. The problem is further exacerbated for false negative rate, which is estimated only over *positive examples* and so its sample sizes is further reduced due to label imbalance.\n",
        "\n",
        "We next visualize the metrics with the corresponding error bars using a custom plotting function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqFPLsATwROr"
      },
      "outputs": [],
      "source": [
        "def plot_group_metrics_with_error_bars(metricframe, metric, error_name):\n",
        "  \"\"\"\n",
        "  Plots the disaggregated `metric` for each group with an associated\n",
        "  error bar. Both metric and the erro bar are provided as columns in the\n",
        "  provided metricframe.\n",
        "  \"\"\"\n",
        "  grouped_metrics = metricframe.by_group\n",
        "  point_estimates = grouped_metrics[metric]\n",
        "  error_bars = grouped_metrics[error_name]\n",
        "  lower_bounds = point_estimates - error_bars\n",
        "  upper_bounds = point_estimates + error_bars\n",
        "\n",
        "  x_axis_names = [str(name) for name in error_bars.index.to_flat_index().tolist()]\n",
        "  plt.vlines(x_axis_names, lower_bounds, upper_bounds, linestyles=\"dashed\", alpha=0.45)\n",
        "  plt.scatter(x_axis_names, point_estimates, s=25)\n",
        "  plt.xticks(rotation=0)\n",
        "  y_start, y_end = np.round(min(lower_bounds), decimals=2), np.round(max(upper_bounds), decimals=2)\n",
        "  plt.yticks(np.arange(y_start, y_end, 0.05))\n",
        "  plt.ylabel(metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsRFpuXfzrUA"
      },
      "outputs": [],
      "source": [
        "plot_group_metrics_with_error_bars(metricframe_unmitigated_w_err, \"selection_rate\", \"selection_err_bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B68Q2ZIgzrcE"
      },
      "outputs": [],
      "source": [
        "plot_group_metrics_with_error_bars(metricframe_unmitigated_w_err, \"false_negative_rate\", \"fnr_err_bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htoUhyirpqZt"
      },
      "outputs": [],
      "source": [
        "plot_group_metrics_with_error_bars(metricframe_unmitigated_w_err, \"balanced_accuracy\", \"bal_acc_err_bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAMhqvycR7eE"
      },
      "source": [
        "As we see above, even accounting for the larger uncertainty in estimating the false negative rate for *Unknown*, this group is experiencing substantially larger false negative rate than other groups and thus experiences the harm of allocation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZqVGZkam1eH"
      },
      "source": [
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dgITdRiD7Yu"
      },
      "source": [
        "# **Mitigating fairness-related harms in ML models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbUSG1jVA06G"
      },
      "source": [
        "We have found that the logistic regression predictor leads to a large difference in false negative rates between the groups. We next look at **algorithmic mitigation strategies** of this fairness issue (and similar ones).\n",
        "\n",
        "*Note that while we currently focus on the training stage of the AI lifecycle mitigation should not be limited to this stage. In fact, we have already discussed mitigation strategies that are applicable at the task definition stage (e.g., checking for construct validity) and data collection stage (e.g., collecting more data).*\n",
        "\n",
        "Within the model training stage, mitigation may occur at different steps relative to model training:\n",
        "\n",
        "* **Preprocessing**: A mitigation algorithm is applied to transform the input data to the training algorithm; for example, some strategies seek to remove and dependence between the input features and sensitive features.\n",
        "\n",
        "* **At training time**: The model is trained by an (optimization) algorithm that seeks to satisfy fairness constraints.\n",
        "\n",
        "* **Postprocessing**: The output of a trained model is transformed to mitigate fairness issues; for example, the predicted probability of readmission is thresholded according to a group-specific threshold.\n",
        "\n",
        "We will now dive into two algorithms: a postprocessing approach and a reductions approach (which is a training-time algorithm). Both of them are in fact **meta-algorithms** in the sense that they act as wrappers around *any* standard (fairness-unaware) machine learning algorithms. This makes them quite versatile in practice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8QycCL0mJj"
      },
      "source": [
        "## Postprocessing with `ThresholdOptimizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRZfSFzcFaXP"
      },
      "source": [
        "**Postprocessing** techniques are a class of unfairness-mitigation algorithms that take an already trained model and a dataset as an input and seek to fit a transformation function to model's outputs to satisfy some (group) fairness constraint(s). They might be the only feasible unfairness mitigation approach when developers cannot influence training of the model, due to practical reasons or due to security or privacy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PgzZkK9Wbni"
      },
      "source": [
        "Here we use the `ThresholdOptimizer` algorithm from Fairlearn, which follows the approach of [Hardt, Price, and Srebro (2016)](https://arxiv.org/abs/1610.02413).\n",
        "\n",
        "`ThresholdOptimizer` takes in an exisiting (possibly pre-fit) machine learning model whose predictions act as a scoring function and identifies a separate thrceshold for each group in order to optimize some specified objective metric (such as **balanced accuracy**) subject to specified fairness constraints (such as **false negative rate parity**). Thus, the resulting classifier is just a suitably thresholded version of the underlying machinelearning model.\n",
        "\n",
        "The constraint **false negative rate parity** requires that all the groups have equal values of false negative rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOovaN7AwDr"
      },
      "source": [
        "To instatiate our `ThresholdOptimizer`, we pass in:\n",
        "\n",
        "*   An existing `estimator` that we wish to threshold.\n",
        "*   The fairness `constraints` we want to satisfy.\n",
        "*   The `objective` metric we want to maximize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8je0grKPWHhy"
      },
      "outputs": [],
      "source": [
        "# Now we instantite ThresholdOptimizer with the logistic regression estimator\n",
        "postprocess_est = ThresholdOptimizer(\n",
        "    estimator=unmitigated_pipeline,\n",
        "    constraints=\"false_negative_rate_parity\",\n",
        "    objective=\"balanced_accuracy_score\",\n",
        "    prefit=True,\n",
        "    predict_method='predict_proba'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDD86L7eCSe0"
      },
      "source": [
        "In order to use the `ThresholdOptimizer`, we need access to the sensitive features **both during training time and once it's deployed**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCHJBB7x1rAK"
      },
      "outputs": [],
      "source": [
        "postprocess_est.fit(X_train_bal, Y_train_bal, sensitive_features=A_train_bal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YscNZsYU1rCY"
      },
      "outputs": [],
      "source": [
        "# Record and evaluate the output of the trained ThresholdOptimizer on test data\n",
        "\n",
        "Y_pred_postprocess = postprocess_est.predict(X_test, sensitive_features=A_test)\n",
        "metricframe_postprocess = MetricFrame(\n",
        "    metrics=metrics_dict,\n",
        "    y_true=Y_test,\n",
        "    y_pred=Y_pred_postprocess,\n",
        "    sensitive_features=A_test\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_izbGv6tQ1KD"
      },
      "source": [
        "We can now inspect how the metric values differ between the postprocessed model and the unmitigated model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9mtWyWc1rH5"
      },
      "outputs": [],
      "source": [
        "pd.concat([metricframe_unmitigated.by_group,\n",
        "           metricframe_postprocess.by_group],\n",
        "           keys=['Unmitigated', 'ThresholdOptimizer'],\n",
        "           axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzPCUFsXPU_S"
      },
      "source": [
        "We next zoom in on differences between the largest and the smallest metric values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsC4v8Ap1rKt"
      },
      "outputs": [],
      "source": [
        "pd.concat([metricframe_unmitigated.difference(),\n",
        "           metricframe_postprocess.difference()],\n",
        "          keys=['Unmitigated: difference', 'ThresholdOptimizer: difference'],\n",
        "          axis=1).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhi_RSxSRoyg"
      },
      "source": [
        "As we see, `ThresholdOptimizer` was able to substantiallydecrease the difference between the values of false negative rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GarQvopkVN2S"
      },
      "source": [
        "Finally, we save the disagregated statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsTehBH2SW7f"
      },
      "outputs": [],
      "source": [
        "metricframe_postprocess.by_group.plot.bar(subplots=True, layout=[1,3], figsize=(12, 4), legend=False, rot=-45, position=1.5)\n",
        "postprocess_performance = figure_to_base64str(plt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umd3slsDmk0d"
      },
      "source": [
        "Next optional section shows that `ThresholdOptimizer` more closely satisfies constraints on the training data than on the test data.\n",
        "\n",
        "### Postprocessing: Correctness check [OPTIONAL SECTION]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs1kWzda8f0z"
      },
      "source": [
        "We can verify that `ThresholdOptimizer` achieves false negative rate parity on the training dataset, meaning that the values of the false negative rate parity with respect to all groups are close on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttOZAVbgmplf"
      },
      "outputs": [],
      "source": [
        "# Record and evaluate the output of the ThresholdOptimizer on the training data\n",
        "\n",
        "Y_pred_postprocess_training = postprocess_est.predict(X_train_bal, sensitive_features=A_train_bal)\n",
        "metricframe_postprocess_training = MetricFrame(\n",
        "    metrics=metrics_dict,\n",
        "    y_true=Y_train_bal,\n",
        "    y_pred=Y_pred_postprocess_training,\n",
        "    sensitive_features=A_train_bal\n",
        ")\n",
        "metricframe_postprocess_training.by_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tty0cmJomp1h"
      },
      "outputs": [],
      "source": [
        "# Evaluate the difference between the largest and smallest value of each metric\n",
        "metricframe_postprocess_training.difference()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvdn7wBhCqXw"
      },
      "source": [
        "The value of `false_negative_rate_difference` on the training data is smaller than on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2vLNUnK_P66"
      },
      "source": [
        "<a name=\"exercise-threshold\"></a>\n",
        "### Exercise: ThresholdOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YUembo02yQ8"
      },
      "source": [
        "In this exercise, we will create a `ThresholdOptimizer` by constraining the *true positive rate* (also known as the *recall score*). For any model, the *true positive rate* + *false negative rate* = 1.\n",
        "\n",
        "By trying to achieve the *true positive rate parity*, we should produce a `ThresholdOptimizer` with the same performance as our original `ThresholdOptimizer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJxE2eMuNF3_"
      },
      "source": [
        "#### 1.) Create a new ThresholdOptimizer with the constraint `true_positive_rate_parity` and objective `balanced_accuracy_score`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD5kQu6gqyl6"
      },
      "outputs": [],
      "source": [
        "thresopt_exercise = ThresholdOptimizer(\n",
        "    estimator=______________,\n",
        "    constraints=____________,\n",
        "    objective=\"balanced_accuracy_score\",\n",
        "    prefit=True,\n",
        "    predict_method='predict_proba'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxdAikCKqyuW"
      },
      "outputs": [],
      "source": [
        "thresopt_exercise.____(X_train_bal, Y_train_bal, sensitive_features=_______)\n",
        "threshopt_pred = thresopt_exercise._________(X_test, sensitive_features=_______)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBLD77OENFN-"
      },
      "source": [
        "#### 2.) Create a new `MetricFrame` object to process the results of this classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B12UNsZErImo"
      },
      "outputs": [],
      "source": [
        "thresop_metricframe = MetricFrame(\n",
        "    metrics=metrics_dict,\n",
        "    y_true=Y_test,\n",
        "    y_pred=____________,\n",
        "    sensitive_features=_______\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEyEie-9NEew"
      },
      "source": [
        "#### 3.) Compare the performance of the two `ThresholdOptimizers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz9mS66YrWka"
      },
      "outputs": [],
      "source": [
        "# Visualize the performance of the new ThresholdOptimizer\n",
        "thresop_metricframe._______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oByEABVGrXWo"
      },
      "outputs": [],
      "source": [
        "# Compare the performance to the original ThresholdOptimizer\n",
        "metricframe_postprocess.______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAUI3LdQbBCs"
      },
      "source": [
        "Similar to many unfairness mitigation approaches, `ThresholdOptimizer` produces randomized classifiers. Next optional section presents a heuristic strategy for converting a randomized `ThresholdOptimizer` into a deterministic one. In our scenario, this heursitic is quite effective and the resulting deterministic classifier has similar performance as the original `ThresholdOptimizer`.\n",
        "\n",
        "### Deployment considerations: Randomized predictions [OPTIONAL SECTION]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlfeHwVC6dna"
      },
      "source": [
        "When we were describing `ThresholdOptimizer` we said that it picks a separate threshold for each group. However, that is not quite correct. In fact,`ThresholdOptimizer`, for each group, picks two thresholds that are close to each other (say `threshold0` and `threshold1`) and then, at deployment time, randomizes between the two: choosing `threshold0` with some probability `p0` and `threshold1` with the remaining probability `p1=1-p0` (the specific probabilities are determined during training; for certain kinds of constraints, three thresholds are considered.)\n",
        "\n",
        "This means that the predictions are randomized. To achieve reproducible randomization, it is possible to provide an argument `random_state` to the `predict` method. However, in some settings, even such reproducible randomization is not acceptable and can be in fact viewed as a fairness issue, because of its arbitrariness.\n",
        "\n",
        "One derandomization heuristic is to replace the two thresholds by their weighted average, i.e., `threshold = p0*threshold0 + p1*threshold1`. That corresponds to the assumption that the values of the scores between the two thresholds are approximately uniformly distributed. Using this heuristic, we derandomize `ThresholdOptimizer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nia26AD2BBKf"
      },
      "source": [
        "The randomized model of the `ThresholdOptimizer` is stored as the field\n",
        "`interpolated_thresholder_` in the fitted ThresholdOptimizer, which is itself a\n",
        "valid estimator of type `InterpolatedThresholder`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w-c22tsZsvj"
      },
      "outputs": [],
      "source": [
        "interpolated = postprocess_est.interpolated_thresholder_\n",
        "interpolated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PkqiznTuL0l"
      },
      "source": [
        "The `interpolation_dict` is a dictionary which assign to each sensitive feature value two thresholds and two respective probabilities. Using our derandomization strategy, we can create a dictionary that represents a deterministic rule:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3a1Mw8koLUa"
      },
      "outputs": [],
      "source": [
        "def create_deterministic(interpolate_dict):\n",
        "  \"\"\"\n",
        "  Creates a deterministic interpolation_dictionary from a randomized\n",
        "  interpolation_dictionary. The determinstic thresholds are created by taking\n",
        "  the weighted combinations of the two randomized thresholds for each sensitive\n",
        "  group.\n",
        "  \"\"\"\n",
        "  deterministic_dict = {}\n",
        "  for (race, operations) in interpolate_dict.items():\n",
        "    op0, op1 = operations[\"operation0\"]._threshold, operations[\"operation1\"]._threshold\n",
        "    p0, p1 = operations[\"p0\"], operations[\"p1\"]\n",
        "    deterministic_dict[race] = Bunch(\n",
        "      p0=0.0,\n",
        "      p1=1.0,\n",
        "      operation0=ThresholdOperation(operator=\">\",threshold=(p0*op0 + p1*op1)),\n",
        "      operation1=ThresholdOperation(operator=\">\",threshold=(p0*op0 + p1*op1))\n",
        "    )\n",
        "  return deterministic_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbynyyy2CKxv"
      },
      "outputs": [],
      "source": [
        "deterministic_dict = create_deterministic(interpolated.interpolation_dict)\n",
        "deterministic_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euqycBW_3EB2"
      },
      "source": [
        "Now, we can create an `InterpolatedThresholder` that uses the same pre-fit estimator, but with derandomized thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3qKiaMiPOaG"
      },
      "outputs": [],
      "source": [
        "deterministic_thresholder = InterpolatedThresholder(estimator=interpolated.estimator,\n",
        "                                                 interpolation_dict=deterministic_dict,\n",
        "                                                 prefit=True,\n",
        "                                                 predict_method='predict_proba')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLNNadraaFMT"
      },
      "outputs": [],
      "source": [
        "deterministic_thresholder.fit(X_train_bal, Y_train_bal, sensitive_features=A_train_bal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPRwWuuWY2Yy"
      },
      "outputs": [],
      "source": [
        "y_pred_postprocess_deterministic = deterministic_thresholder.predict(X_test, sensitive_features=A_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3834uLWeXWO"
      },
      "outputs": [],
      "source": [
        "mf_deterministic = MetricFrame(\n",
        "    metrics=metrics_dict,\n",
        "    y_true=Y_test,\n",
        "    y_pred=y_pred_postprocess_deterministic,\n",
        "    sensitive_features=A_test\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tnGhMYzDloo"
      },
      "source": [
        "Now compare the two models in terms of their disaggregated metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIkoljGTeXZ0"
      },
      "outputs": [],
      "source": [
        "mf_deterministic.by_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVimPa2nlFpv"
      },
      "outputs": [],
      "source": [
        "metricframe_postprocess.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOi4dwLKECHS"
      },
      "source": [
        "The differences are generally small except for the *Unknown* group, whose false negative rate goes down and balanced accuracy goes up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU_rncBQ0lab"
      },
      "source": [
        "## Reductions approach with `ExponentiatedGradient`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpyBnqNLZ-w_"
      },
      "source": [
        "With the `ThresholdOptimizer`, we took a fairness-unaware model and transformed the model's decision boundary to satisfy our fairness constraints. One limitation of `ThresholdOptimizer` is that it needs access to the sensitive features at deployment time.\n",
        "\n",
        "In this section, we will show how to use the _reductions_ approach of [Agarwal et. al (2018)](https://arxiv.org/abs/1803.02453) to obtain a model that satisfies the fairness constraints, but does not need access to sensitive features at deployment time.\n",
        "\n",
        "Terminology \"reductions\" refers to another kind of a wrapper approach, which instead of wrapping an already trained model, wraps any standard classification or regression algorithm, such as\n",
        "`LogisticRegression`. In other words, an input to a reduction algorithm is an object that supports training on any provided (weighted) dataset. In addition, a reduction algorithm receives a data set that includes sensitive features. The goal, like with post-processing, is to optimize a performance metric (such as classification accuracy) subject to fairness constraints (such as an upper bound on differences between false negative rates).\n",
        "\n",
        "The main reduction algorithm algorithm in Fairlearn is `ExponentiatedGradient`. It creates a sequence of reweighted datasets and retrains the wrapped model on each of them. The\n",
        "retraining process is guaranteed to find a model that satisfies the fairness constraints while optimizing the performance metric.\n",
        "\n",
        "The model returned by `ExponentiatedGradient` consists of several inner models, returned by the wrapped estimator. At deployment time, `ExponentiatedGradient` randomizes among these models according to a specific probability weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvT_qeduHCn8"
      },
      "source": [
        "To instantiate an `ExponentiatedGradient` model, we pass in two parameters:\n",
        "\n",
        "*   A base `estimator` (an object that supports training)\n",
        "*   Fairness `constraints` (an object of type `Moment`).\n",
        "\n",
        "The constraints supported by `ExponentiatedGradient` are more general than those supported by `ThresholdOptimizer`. For example, rather than requiring that false negative rates be equal, it is possible to specify the maxium allowed difference or ratio between the largest and the smallest value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToZdrYen0tJZ"
      },
      "outputs": [],
      "source": [
        "expgrad_est = ExponentiatedGradient(\n",
        "    estimator=LogisticRegression(max_iter=1000, random_state=random_seed),\n",
        "    constraints=TruePositiveRateParity(difference_bound=0.02)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLFk3SCxrskU"
      },
      "source": [
        "The constraints above are expressed for the true positive parity, they require that the difference between the largest and the smallest true positive rate (TPR) across all groups be at most 0.02. Since false negative rate (FNR) is equal to 1-TPR, this is equivalent to requiring that the difference between the largest and smallest FNR be at most 0.02."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFSF5Wn-3M-H"
      },
      "outputs": [],
      "source": [
        "# Fit the exponentiated gradient model\n",
        "expgrad_est.fit(X_train_bal, Y_train_bal, sensitive_features=A_train_bal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCeOlKFDQZZ"
      },
      "source": [
        "Similarly to `ThresholdOptimizer` the predictions of `ExponentiatedGradient` models are randomized. If we want to assure reproducible results, we can pass  `random_state` to the `predict` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYz7GAqf4cbp"
      },
      "outputs": [],
      "source": [
        "# Record and evaluate predictions on test data\n",
        "\n",
        "Y_pred_reductions = expgrad_est.predict(X_test, random_state=random_seed)\n",
        "metricframe_reductions = MetricFrame(\n",
        "    metrics=metrics_dict,\n",
        "    y_true=Y_test,\n",
        "    y_pred=Y_pred_reductions,\n",
        "    sensitive_features=A_test\n",
        ")\n",
        "metricframe_reductions.by_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idYvm9lq4mh3"
      },
      "outputs": [],
      "source": [
        "# Evaluate the difference between the largest and smallest value of each metric\n",
        "metricframe_reductions.difference()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpxYOozouVx9"
      },
      "source": [
        "While there is a decrease in the false negative rate difference from the unmitigated model, this decrease is not as substantial as with `ThresholdOptimizer`. Note, however, that `ThresholdOptimizer` was able to use the sensitive feature (i.e., race) at deployment time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzTeHhWG4nJJ"
      },
      "source": [
        "### Explore individual predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7qCmHeYKWGp"
      },
      "source": [
        "During the training process, the `ExponentiatedGradient` algorithm iteratively trains multiple inner models on a reweighted training dataset. The algorithm stores each of these predictors and then randomizes among them at deployment time.\n",
        "\n",
        "In many applications, the randomization is undesirable, and also using multiple inner models can pose issues for interpretability. However, the inner models that `ExponentiatedGradient` relies on span a variety of fairness-accuracy trade-offs, and they could be considered for stand-alone deployment: addressing the randomization and interpretability issues, while possibly offering additional flexibility thanks to a variety of trade-offs.\n",
        "\n",
        "In this section explore the performance of the individual predictors learned by the `ExponentiatedGradient` algorithm. First, note that since the base estimator was `LogisticRegression` all these predictors are different logistic regression models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II_YtUZg3Ue4"
      },
      "outputs": [],
      "source": [
        "predictors = expgrad_est.predictors_\n",
        "predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3Bh2IVm4Ynj"
      },
      "outputs": [],
      "source": [
        "# Collect predictions by all predictors and calculate balanced error\n",
        "# as well as the false negative difference for all of them\n",
        "\n",
        "sweep_preds = [clf.predict(X_test) for clf in predictors]\n",
        "balanced_error_sweep = [1-balanced_accuracy_score(Y_test, Y_sweep) for Y_sweep in sweep_preds]\n",
        "fnr_diff_sweep = [false_negative_rate_difference(Y_test, Y_sweep, sensitive_features=A_test) for Y_sweep in sweep_preds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9TKGsNY8Myu"
      },
      "outputs": [],
      "source": [
        "# Show the balanced error / fnr difference values of all predictors on a raster plot\n",
        "\n",
        "plt.scatter(balanced_error_sweep, fnr_diff_sweep, label=\"ExponentiatedGradient - Iterations\")\n",
        "for i in range(len(predictors)):\n",
        "  plt.annotate(str(i), xy=(balanced_error_sweep[i]+0.001, fnr_diff_sweep[i]+0.001))\n",
        "\n",
        "# Also include in the plot the combined ExponentiatedGradient model\n",
        "# as well as the three previously fitted models\n",
        "\n",
        "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred_reductions),\n",
        "            false_negative_rate_difference(Y_test, Y_pred_reductions, sensitive_features=A_test),\n",
        "            label=\"ExponentiatedGradient - Combined model\")\n",
        "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred),\n",
        "            false_negative_rate_difference(Y_test, Y_pred, sensitive_features=A_test),\n",
        "            label=\"Unmitigated\")\n",
        "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred_postprocess),\n",
        "            false_negative_rate_difference(Y_test, Y_pred_postprocess, sensitive_features=A_test),\n",
        "            label=\"ThresholdOptimizer\")\n",
        "plt.scatter(1-balanced_accuracy_score(Y_test, y_pred_postprocess_deterministic),\n",
        "            false_negative_rate_difference(Y_test, y_pred_postprocess_deterministic, sensitive_features=A_test),\n",
        "            label=\"ThresholdOptimizer (DET)\")\n",
        "\n",
        "plt.xlabel(\"Balanced Error Rate\")\n",
        "plt.ylabel(\"False Negative Rate Difference\")\n",
        "plt.legend(bbox_to_anchor=(1.9,1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MozjJKkZqz_"
      },
      "source": [
        "<a name=\"exercise-reductions\"></a>\n",
        "### Exercise: Train an `ExponentiatedGradient` model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLeGnB4juJsM"
      },
      "source": [
        "In this section, we will explore how changing the base model for the `ExponentiatedGradient` affects the overall performance of the classifier.\n",
        "\n",
        "We will instatiate a new `ExponentiatedGradient` classifier with a base `HistGradientBoostingClassifer` estimator. We will use the same `difference_bound` as above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghjfKhtB3Kl9"
      },
      "source": [
        "1.) First, let's create our new `ExponentiatedGradient` instance in the cells below and fit it to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNjDh4JUAc6i"
      },
      "outputs": [],
      "source": [
        "# Create ExponentiatedGradient instance here\n",
        "expgrad_exercise = ExponentiatedGradient(\n",
        "    estimator=_______,\n",
        "    constraints=TruePositiveRateParity(difference_bound=____)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFIAEkEBAc9P"
      },
      "outputs": [],
      "source": [
        "# Fit the new instance to the balanced training dataset\n",
        "expgrad_exercise.fit(________, _________, sensitive_features=________)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H_UDlAs3M-D"
      },
      "source": [
        "2.) Now, let's compute the performance of the `ExponentiatedGradient` model and compare it with the performance of `ExponentiatedGradient` model with logistic regression as base estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qkfth4ZZBQV"
      },
      "outputs": [],
      "source": [
        "# Save the predictions and report the disagregated metrics\n",
        "# of the exponantiated gradient model\n",
        "Y_expgrad_exercise = expgrad_exercise.predict(X_test)\n",
        "mf_expgrad_exercise = MetricFrame(\n",
        "    metrics=metrics_dict,\n",
        "    y_true=Y_test,\n",
        "    y_pred=_______,\n",
        "    sensitive_features=________\n",
        ")\n",
        "mf_expgrad_exercise.______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4noXEU6vEDC"
      },
      "outputs": [],
      "source": [
        "# Compare with the disaggregated metric values of the\n",
        "# exponentiated gradient model based on logistic regression\n",
        "metricframe_reductions.____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Boo771yFUyJ7"
      },
      "source": [
        "3.) Next, calculate the balanced error rate and false negative rate difference of each of the inner models learned by this new `ExponentiatedGradient` classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jws7w2z6RWUM"
      },
      "outputs": [],
      "source": [
        "# Save the inner predictors of the new model\n",
        "predictors_exercise = expgrad_exercise.predictors_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVTqrrSRAtGc"
      },
      "outputs": [],
      "source": [
        "# Compute the balanced error rate and false negative rate difference for each of the predictors on the test data.\n",
        "balanced_error_exercise = [(1 - ______(Y_test, pred.predict(X_test))) for pred in predictors_exercise]\n",
        "false_neg_exercise = [(______(Y_test, pred.predict(X_test), sensitive_features=_____)) for pred in predictors_exercise]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aos3gdHjDQEi"
      },
      "source": [
        "4.) Finally, let's plot the performances of these individual inner models. In the below cells, plot the individual inner predictors against the performance of their corresponding exponentiated gradient model as well as the unmitigated logistic regression model, and the `ThresholdOptimizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxTjb8CeZqPI"
      },
      "outputs": [],
      "source": [
        "# Plot the individual predictors against the Unmitigated Model and the ThresholdOptimizer\n",
        "plt.scatter(balanced_error_exercise, false_neg_exercise,\n",
        "            label=\"ExponentiatedGradient - Iterations - Exercise\")\n",
        "for i in range(len(predictors_exercise)):\n",
        "  plt.annotate(str(i), xy=(balanced_error_exercise[i]+0.001, false_neg_exercise[i]+0.001))\n",
        "\n",
        "plt.scatter(1-balanced_accuracy_score(Y_test, Y_expgrad_exercise),\n",
        "            false_negative_rate_difference(Y_test, Y_expgrad_exercise, sensitive_features=A_test),\n",
        "            label=\"ExponentiatedGradient - Combined - Exercise\")\n",
        "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred),\n",
        "            false_negative_rate_difference(Y_test, Y_pred, sensitive_features=A_test),\n",
        "            label=\"Unmitigated\")\n",
        "plt.scatter(1-balanced_accuracy_score(Y_test, Y_pred_postprocess),\n",
        "            false_negative_rate_difference(Y_test, Y_pred_postprocess, sensitive_features=A_test),\n",
        "            label=\"ThresholdOptimizer\")\n",
        "\n",
        "plt.xlabel(\"Balanced Error Rate\")\n",
        "plt.ylabel(\"False Negative Rate Difference\")\n",
        "plt.legend(bbox_to_anchor=(1.9,1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyknfgrsW1gi"
      },
      "source": [
        "## Comparing performance of different techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vee-7c2Tw33O"
      },
      "source": [
        "Now we have covered two different class of techniques for mitigating the fairness-related harms we found in our fairness-unaware model. In this section, we will compare the performance of the models we trained above across our key metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEXnEeLl7mgc"
      },
      "source": [
        "#### Model performance - by group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNyCZxHJuXZV"
      },
      "outputs": [],
      "source": [
        "def plot_technique_comparison(mf_dict, metric):\n",
        "  \"\"\"\n",
        "  Plots a specified metric for a given dictionary of MetricFrames.\n",
        "  \"\"\"\n",
        "  mf_dict = {k:v.by_group[metric] for (k,v) in mf_dict.items()}\n",
        "  comparison_df = pd.DataFrame.from_dict(mf_dict)\n",
        "  comparison_df.plot.bar(figsize=(12, 6), legend=False)\n",
        "  plt.title(metric)\n",
        "  plt.xticks(rotation=0, ha='center');\n",
        "  plt.legend(bbox_to_anchor=(1.01,1), loc='upper left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dNb-kI3uHzM"
      },
      "outputs": [],
      "source": [
        "test_dict = {\n",
        "    \"Reductions\": metricframe_reductions,\n",
        "    \"Unmitigated\": metricframe_unmitigated,\n",
        "    \"Postprocessing\": metricframe_postprocess,\n",
        "    \"Postprocessing (DET)\": mf_deterministic\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SweXBa-vEWFM"
      },
      "outputs": [],
      "source": [
        "plot_technique_comparison(test_dict, \"false_negative_rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJOwW9db3wKe"
      },
      "outputs": [],
      "source": [
        "plot_technique_comparison(test_dict, \"balanced_accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dACpOpm67K-m"
      },
      "outputs": [],
      "source": [
        "plot_technique_comparison(test_dict, \"selection_rate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYkgAggpW11N"
      },
      "source": [
        "\n",
        "\n",
        "#### Model performance - overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3Rwe98m6Pv2"
      },
      "outputs": [],
      "source": [
        "overall_df = pd.DataFrame.from_dict({\n",
        "    \"Unmitigated\": metricframe_unmitigated.overall,\n",
        "    \"Postprocessing\": metricframe_postprocess.overall,\n",
        "    \"Postprocessing (DET)\": mf_deterministic.overall,\n",
        "    \"Reductions\": metricframe_reductions.overall\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Q-lYTa6PzX"
      },
      "outputs": [],
      "source": [
        "overall_df.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUaAejwa6P3F"
      },
      "outputs": [],
      "source": [
        "overall_df.transpose().plot.bar(subplots=True, layout= [1,3], figsize=(12, 5), legend=False, rot=-45, position=1.5);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIGS8f2M2Afu"
      },
      "outputs": [],
      "source": [
        "difference_df = pd.DataFrame.from_dict({\n",
        "    \"Unmitigated\": metricframe_unmitigated.difference(),\n",
        "    \"Postprocessing\": metricframe_postprocess.difference(),\n",
        "    \"Postprocessing (DET)\": mf_deterministic.difference(),\n",
        "    \"Reductions\": metricframe_reductions.difference()\n",
        "}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pBkP7QCDVs6"
      },
      "outputs": [],
      "source": [
        "difference_df.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qXEkByRDkK0"
      },
      "outputs": [],
      "source": [
        "difference_df.T.plot.bar(subplots=True, layout= [1,3], figsize=(12, 5), legend=False, rot=-45, position=1.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S6CIoJ-W1jH"
      },
      "source": [
        "### Randomized predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzdFLJ9BA15F"
      },
      "source": [
        "Both the `ExponentiatedGradient` and the `ThresholdOptimizer` yield randomized predictions (may return different result given the same instance). Due to legal regulations or other concerns, a practitioner may not be able to deploy a randomized model. To address these restrictions:\n",
        "\n",
        "*   We created a deterministic predictor based on the randomized thresholds learned by the `ThresholdOptimizer`. This deteministic predictor achieved similar performance as the `ThresholdOptimizer`.\n",
        "*   For the `ExponentiatedGradient` model, we can deploy one of the deterministic inner models rather than the overall `ExponentiatedGradient` model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9dvkQvKW1lv"
      },
      "source": [
        "### Access to sensitive features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L56tawsAW14B"
      },
      "source": [
        "\n",
        "\n",
        "*   The `ThresholdOptimizer` model requires access to the sensitive features during BOTH training time and once deployed. If you do not have access to the sensitive features once the model is deployed, you will not be able to use the `ThresholdOptimizer`.\n",
        "*   The `ExponentiatedGradient` model requires access to the sensitive features ONLY during training time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQOR2R3XVa-U"
      },
      "source": [
        "# Model cards for model reporting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzFzSHrQBAgi"
      },
      "source": [
        "_Note: The Python code in this section works in Google Colab, but it does not work on all local environments that we tested._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RAXJDoyVbBT"
      },
      "source": [
        "[Mitchell et al. (2019)](https://arxiv.org/abs/1810.03993) proposed the *model cards* framework for documenting and reporting model training details and deployment considerations. A _model card_ documents, for example, training and evaluation dataset summaries, ethical considerations, and quantitative performance results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP_tklbmMoFH"
      },
      "source": [
        "### Fill out the model card [OPTIONAL SECTION]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfI9oEwIkS6v"
      },
      "source": [
        "In this section, we will create a model card for one of the models we trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OViYhOAHPftT"
      },
      "outputs": [],
      "source": [
        "mct = ModelCardToolkit()\n",
        "model_card = mct.scaffold_assets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnr6aRjIF_t6"
      },
      "source": [
        "The first section of the Model Card is the _model details_ section. In _model details_, we fill in some basic information for our model.\n",
        "\n",
        "\n",
        "*   _Name_: A name for the model\n",
        "*   _Overview_: A brief description of the model and its intended use case\n",
        "*   _Owners_: Name of individual(s) or group who created the model.\n",
        "*   _References_: Any external links or references\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKvUW25cPewp"
      },
      "outputs": [],
      "source": [
        "model_card.model_details.name = \"Diabetes Re-Admission Risk model\"\n",
        "model_card.model_details.overview = \"This model predicts whether a patient will be re-admitted into a hospital within 30 days.\"\n",
        "model_card.model_details.owners = [{\n",
        "    \"name\": \"Fairlearn Team\",\n",
        "    \"contact\": \"https://fairlearn.org/\"\n",
        "}]\n",
        "model_card.model_details.reference = [\n",
        "    \"https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\"\n",
        "]\n",
        "model_card.model_details.version.name = \"v1.0\"\n",
        "model_card.model_details.version.date = str(date.today())\n",
        "model_card.model_details.license = \"MIT License\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avEdkdzAPe2S"
      },
      "outputs": [],
      "source": [
        "model_card.considerations.use_cases = [\"High-Risk Patient Care Management\"]\n",
        "model_card.considerations.users = [\"Medical Professionals\",  \"ML Researchers\"]\n",
        "model_card.considerations.limitations = [\n",
        "    \"\"\"\n",
        "    This model will not generalize to hospitals outside of the United States. Features, such as those encoding insurance\n",
        "    information, are inherently tied to the U.S healthcare system.\n",
        "    In addition, this model is intended for patients who are admitted into U.S. hospitals for diabetes-related illnessess.\n",
        "    \"\"\"\n",
        "]\n",
        "model_card.considerations.ethical_considerations = [{\n",
        "    \"name\": (\"Low sample sizes of certain racial groups could lead to poorer performance on these groups\"),\n",
        "    \"mitigation_strategy\": \"Collect additional data points from more hospitals.\"\n",
        "}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orPNYAZFwGVM"
      },
      "source": [
        "The next two sections of the model card are meant to provide the reader with information about the data used to train and evaluate the model. For each of these sections, we provide a brief `description` of the data and then submit a `visualization` of the distribution of labels in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khWJN_vqm35e"
      },
      "outputs": [],
      "source": [
        "model_card.model_parameters.data.train.graphics.description = (\n",
        "    f\"{X_train_bal.shape[0]} rows with {X_train_bal.shape[1]} features. \"\n",
        "    f\"The original training data set was undersampled to allow for an equal number of positive and negative labeled instances.\"\n",
        ")\n",
        "\n",
        "model_card.model_parameters.data.train.graphics.collection = [\n",
        "    {\"name\": \"Sensitive Features\", \"image\": sensitive_train},\n",
        "    {\"name\": \"Target Label\", \"image\": outcome_train}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3fiatJ6nGQf"
      },
      "outputs": [],
      "source": [
        "model_card.model_parameters.data.eval.graphics.description = (\n",
        "    f\"{X_test.shape[0]} rows with {X_test.shape[1]} columns\"\n",
        ")\n",
        "\n",
        "model_card.model_parameters.data.eval.graphics.collection = [\n",
        "    {\"name\": \"Sensitive Features\", \"image\": sensitive_test},\n",
        "    {\"name\": \"Target Label\", \"image\": outcome_test}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frGlrXHAyn2C"
      },
      "source": [
        "In the last section, we fill out the `quantitative_analysis` section where we describe the model's performance metrics on the evaluation dataset. In particular, we want to report the model's disagregated performance with respect to our three metrics including false negative rate, which quantifies fairness-related harms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cyHIMlAJqod"
      },
      "outputs": [],
      "source": [
        "def metricframe_to_dictionary(mframe, feature_name):\n",
        "    \"\"\"\n",
        "    Converts a MetricFrame into a Dictionary object that can be accepted by the Model Card's\n",
        "    Quantitative Analysis section.\n",
        "    \"\"\"\n",
        "    group_metrics = mframe.by_group[feature_name].reset_index()\n",
        "    group_metrics = group_metrics.melt(id_vars=\"race\", var_name=\"type\", value_vars=feature_name).rename(columns={\"race\":\"slice\"})\n",
        "    return group_metrics.to_dict(orient=\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kETWDtN68Gmq"
      },
      "outputs": [],
      "source": [
        "model_card.quantitative_analysis.graphics.description = (\n",
        "    f\"These graphs show the models performance on the test dataset for disagregated racial categories.\"\n",
        ")\n",
        "model_card.quantitative_analysis.performance_metrics = metricframe_to_dictionary(metricframe_postprocess, \"false_negative_rate\")\n",
        "model_card.quantitative_analysis.graphics.collection = [\n",
        "    {\"name\": \"ThresholdOptimizer\", \"image\": postprocess_performance}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvL2mHlwp-l0"
      },
      "source": [
        "Finally, we pass our filled-out `model_card` to the `mct` object to generate an HTML version of the `model_card` that can be rendered within a Jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-j_4mBaPe5z"
      },
      "outputs": [],
      "source": [
        "mct.update_model_card_json(model_card)\n",
        "html_modelcard = mct.export_format()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkuNE_MvMy7P"
      },
      "source": [
        "### Display the model card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89kqC0Jj9D6O"
      },
      "outputs": [],
      "source": [
        "display.HTML(html_modelcard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j9WzWcCVbZV"
      },
      "source": [
        "# Discussion and conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtvW54LtB8hp"
      },
      "source": [
        "In this tutorial we have explored in depth a health care scenario through all stages of the AI lifecycle except the model deployment stage. We have seen how fairness-related harms can arise at the stage of task definition, data collection, model training, and model evaluation. We have also seen how to use a variety of tools and practices, such as datasheets for datasets, Fairlearn, and model cards.\n",
        "\n",
        "Once the model is deployed, it is important to continue monitoring the key metrics to assess any performance difference as well as the potential for fairness related harms. As you learn more about how the model is used, you may need to revise the fairness metrics, update the model, consider additional sensitive features, update the task definition, or collect new data.\n",
        "\n",
        "Although we used a variety of software tools, fairness is a sociotechnical challenge, so mitigations cannot be purely technical, and need to be supported by processes and practices, including government regulation and organizational incentives.\n",
        "\n",
        "If you would like to learn more about fairness of AI systems, or to contribute to Fairlearn, we welcome you to join our community. Fairlearn is built and maintained by contributors with a variety of backgrounds and expertise.\n",
        "\n",
        "Further resources can also be found [on our website](https://fairlearn.org/main/user_guide/further_resources.html)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4MMbl3u7-J-a",
        "ZVSerRqDG3we"
      ],
      "name": "SciPy 2021 Tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "a025db62d48a12d86b0e6b0cb53f59776c5e11a448915a1ba45134646da53519"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}